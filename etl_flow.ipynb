{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f005e04-9796-4428-a128-b70ca0b21e4c",
   "metadata": {},
   "source": [
    "# ETL Flow GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18405b17-d473-4f5b-a49c-f5bd6d28158f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c295e4-6a60-4209-a57c-93ec40c50d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys,os,inspect\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from cronsim import CronSim\n",
    "import argparse\n",
    "import json\n",
    "import ast\n",
    "import pygsheets\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "#import PySimpleGUIWeb as sg\n",
    "import pathlib\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import re\n",
    "from shlex import split\n",
    "import webbrowser\n",
    "import subprocess\n",
    "import collections\n",
    "import pyglet,tkinter\n",
    "from pyglet import font\n",
    "font.add_file('/etc/fonts/fonts/CENTAUR.TTF')\n",
    "font='Courier 10 bold '\n",
    "bicHome = \"/home/joe/bic_etl/\"\n",
    "numWindows=0\n",
    "headerStore = {}\n",
    "\n",
    "colorPairs = [[\"#D6EAF8\",\"#85C1E9\"],[\"#b3f0ff\",\"#33d6ff\"],[\"#D5F5E3\",\"#A3E4D7\"],[\"#FCF3CF\",\"#F7DC6F\"]]\n",
    "windowsOpen = {}\n",
    "windowsOpen[\"main\"] = []\n",
    "windowsOpen[\"unique\"] = []\n",
    "\n",
    "\n",
    "import requests\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "global datasets\n",
    "\n",
    "if sys.platform == \"linux\":\n",
    "    path=\"/home/joe/work/Logs/logs\"\n",
    "else:\n",
    "    path= \"\\\\\\\\wsl.localhost\\\\Ubuntu\\\\home\\\\joe\\\\work\\\\Logs\\\\\"\n",
    "\n",
    "logging = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b9628-c8fb-41a1-8abb-1bc5cc9be430",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6bfb2-7e71-4707-9d4f-8e7a4ef54e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecentErrorsNew():\n",
    "    global ids,errorsByDate\n",
    "    files = []\n",
    "    print(path)\n",
    "    ids={}\n",
    "    for x in os.listdir(path):\n",
    "            if re.findall(\"^log.json*\",x):\n",
    "                 # files2Process.append(x)\n",
    "                 files.append(x)\n",
    "\n",
    "    x = datetime.today() - timedelta(days=30)\n",
    "    monthM = x.month  \n",
    "    yearM = x.year\n",
    "    ff=f\"log_backup_{yearM}_{monthM}.json\"\n",
    "    files.append(ff)\n",
    "    # files=[\"log.json.0\"]\n",
    "    errorsByDate = {}\n",
    "    errorsByName = {}\n",
    "    found2=False\n",
    "    bad = []\n",
    "    tim=0\n",
    "    nfile=0\n",
    "   # print(sorted(files))\n",
    "    for file in sorted(files):\n",
    "        print(\"Processing \",file)\n",
    "        try:\n",
    "            jsf = open(f\"{path}/{file}\")\n",
    "            found=True\n",
    "        except:\n",
    "            print(f\"File not found {path}/{file}\")\n",
    "            sg.Popup(f\"File NOT FOUND, Please Download it {path}/{file}\")\n",
    "            found=False\n",
    "           \n",
    "\n",
    "        if found:  \n",
    "                log=[]\n",
    "                nline=0\n",
    "                nfile+=1\n",
    "                for line in jsf:\n",
    "                    try: \n",
    "                         xl=line.lower()\n",
    "\n",
    "                         if \"error\" in xl and \"metadata\" not in xl:\n",
    "                    #     if \"error\" in xl :\n",
    "                             log.append(ast.literal_eval(line))\n",
    "                    except Exception as err:\n",
    "                       print(\"ERROR \",err)\n",
    "                       bad.append(line)\n",
    "                       print(\"BAD \",line)\n",
    "                    nline+=1\n",
    "                    \n",
    "                    \n",
    "                if  len(log) > 0 and  found2 == False:\n",
    "                    msg = log[0]\n",
    "                    time0 = datetime.strptime(msg['time'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    name0 = msg['name'].strip()\n",
    "                    found2=True\n",
    "                   # print(\"TIME0 \",file,time0)\n",
    "                    \n",
    "             #   tim=0\n",
    "                if found2:\n",
    "                  for line in log:\n",
    "                  #  print(line['time'],line['name'])\n",
    "                    time = line['time']\n",
    "                    time1 = datetime.strptime(time, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    diff = time1-time0\n",
    "                    name1 = line['name'].strip()\n",
    "                  #  print(name0,name1,time0,time1,diff.total_seconds())\n",
    "                    if name1 == name0 and  abs(diff.total_seconds()) < 1:\n",
    "                        same=True\n",
    "                      #  print(\"    \",tim,name0,name1)\n",
    "                    else:\n",
    "                        tim+=1\n",
    "                    name0=name1\n",
    "                    time0=time1\n",
    "                    \n",
    "                    spl = time.split(\"T\")\n",
    "                    time = spl[0]\n",
    "                    name = line['name'].strip()\n",
    "\n",
    "                    msg= line['msg'].strip()\n",
    "\n",
    "                    emsg = \"\"\n",
    "                    if \"err\" in line:\n",
    "                        emsg = line['err']\n",
    "                #        msg+= f\"\\n\\n-------------\\n{err}\"\n",
    "\n",
    "\n",
    "                    uid = f\"{line['time']}{msg[1:20]}\"\n",
    "                    if uid in ids:\n",
    "                        ids[uid]+=1\n",
    "                    else:\n",
    "                        ids[uid]=1\n",
    "\n",
    "                    stitle=\"\"\n",
    "                    xm = msg.find(\"Full ETL failure for \")\n",
    "                    if xm > -1:\n",
    "                       stitle = msg[xm+20:].split(\":\")[0]\n",
    "                       stitle=stitle.replace(\" at load\",\"\")\n",
    "\n",
    "                    xm = msg.find(\"Error Loading \")\n",
    "                    if xm > -1:\n",
    "                       stitle = msg[xm+14:]\n",
    "\n",
    "                    xm = msg.find(\"Error Extracting \")\n",
    "                    if xm > -1:\n",
    "                       stitle=msg[xm+16:]\n",
    "\n",
    "                    xm = msg.find(\"Error Transforming \")\n",
    "                    if xm > -1:\n",
    "                       stitle=msg[xm+18:]\n",
    "\n",
    "                #   \n",
    "                    titl=\"\"\n",
    "                    s4x4=\"\"\n",
    "                    w4x4=\"\"\n",
    "                    try:\n",
    "                        s4x4s = re.findall(\"[\\w]{3,4}-[\\w]{3,4}\",line['msg'])\n",
    "\n",
    "\n",
    "                        if len(s4x4s) > 0:\n",
    "                          for s4x4 in s4x4s:\n",
    "                             if s4x4 in xrefsBy4x4:\n",
    "                                titl = xrefsBy4x4[s4x4]\n",
    "                                w4x4=s4x4\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        titl=\"\"\n",
    "\n",
    "\n",
    "                    if tim in errorsByDate:\n",
    "                        errorsByDate[tim]['name'].append(name)\n",
    "                        errorsByDate[tim]['msg'].append(msg)\n",
    "                        errorsByDate[tim]['emsg'].append(emsg)\n",
    "                        errorsByDate[tim]['line'].append(line)\n",
    "                        errorsByDate[tim]['file'].append(file)\n",
    "                        errorsByDate[tim]['title'].append(titl)\n",
    "                        errorsByDate[tim]['stitle'].append(stitle)\n",
    "                        errorsByDate[tim]['4x4'].append(w4x4)\n",
    "                        errorsByDate[tim]['uid'].append(uid)\n",
    "                        errorsByDate[tim]['time'].append(time)\n",
    "                        \n",
    "\n",
    "                    else:\n",
    "                        errorsByDate[tim] = {}\n",
    "                        errorsByDate[tim]['name'] = []\n",
    "                        errorsByDate[tim]['msg'] = []\n",
    "                        errorsByDate[tim]['emsg'] = []              \n",
    "                        errorsByDate[tim]['line'] = []\n",
    "                        errorsByDate[tim]['file'] = []\n",
    "                        errorsByDate[tim]['title'] = []\n",
    "                        errorsByDate[tim]['stitle'] = []\n",
    "                        errorsByDate[tim]['4x4'] = []\n",
    "                        errorsByDate[tim]['uid'] = []\n",
    "                        errorsByDate[tim]['time'] = []\n",
    "                        \n",
    "\n",
    "                        errorsByDate[tim]['name'].append(name)\n",
    "                        errorsByDate[tim]['msg'].append(msg)\n",
    "                        errorsByDate[tim]['emsg'].append(emsg)            \n",
    "                        errorsByDate[tim]['line'].append(line)\n",
    "                        errorsByDate[tim]['file'].append(file)\n",
    "                        errorsByDate[tim]['title'].append(titl)\n",
    "                        errorsByDate[tim]['stitle'].append(stitle)\n",
    "                        errorsByDate[tim]['4x4'].append(w4x4)\n",
    "                        errorsByDate[tim]['uid'].append(uid)\n",
    "                        errorsByDate[tim]['time'].append(time)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "                    if name in errorsByName:\n",
    "                            errorsByName[name]['time'].append(tim)\n",
    "                            errorsByName[name]['msg'].append(msg)\n",
    "                            errorsByName[name]['line'].append(line)\n",
    "                            errorsByName[name]['file'].append(file)\n",
    "                            errorsByName[name]['title'].append(titl)\n",
    "                    else:\n",
    "                        errorsByName[name] = {}\n",
    "                        errorsByName[name]['time'] = []\n",
    "                        errorsByName[name]['msg'] = []\n",
    "                        errorsByName[name]['line'] = []\n",
    "                        errorsByName[name]['file'] = []\n",
    "                        errorsByName[name]['title'] = []\n",
    "\n",
    "                        errorsByName[name]['time'].append(tim)\n",
    "                        errorsByName[name]['msg'].append(msg)\n",
    "                        errorsByName[name]['line'].append(line)\n",
    "                        errorsByName[name]['file'].append(file)\n",
    "                        errorsByName[name]['title'].append(titl)\n",
    "\n",
    "#                 except Exception as err:\n",
    "#                     print(err)\n",
    "#                     print(line)\n",
    "    return errorsByDate,errorsByName\n",
    "\n",
    "\n",
    "def getRecentErrors():\n",
    "    global ids,errorsByDate\n",
    "    files = []\n",
    "    print(path)\n",
    "    ids={}\n",
    "    for x in os.listdir(path):\n",
    "            if re.findall(\"^log.json*\",x):\n",
    "                 # files2Process.append(x)\n",
    "                 files.append(x)\n",
    "\n",
    "    x = datetime.today() - timedelta(days=30)\n",
    "    monthM = x.month  \n",
    "    yearM = x.year\n",
    "    ff=f\"log_backup_{yearM}_{monthM}.json\"\n",
    "    files.append(ff)\n",
    "    # files=[\"log.json.0\"]\n",
    "    errorsByDate = {}\n",
    "    errorsByName = {}\n",
    "    print(files)\n",
    "    bad = []\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        try:\n",
    "            jsf = open(f\"{path}/{file}\")\n",
    "            found=True\n",
    "        except:\n",
    "            print(f\"File not found {path}/{file}\")\n",
    "            sg.Popup(f\"File NOT FOUND, Please Download it {path}/{file}\")\n",
    "            found=False\n",
    "           \n",
    "\n",
    "        if found:  \n",
    "                log=[]\n",
    "                nline=0\n",
    "                for line in jsf:\n",
    "                    try: \n",
    "                         xl=line.lower()\n",
    "\n",
    "                         if \"error\" in xl and \"metadata\" not in xl:\n",
    "                    #     if \"error\" in xl :\n",
    "                             log.append(ast.literal_eval(line))\n",
    "                    except Exception as err:\n",
    "                       print(\"ERROR \",err)\n",
    "                       bad.append(line)\n",
    "                       print(\"BAD \",line)\n",
    "                    nline+=1\n",
    "\n",
    "                for line in log:\n",
    "                  #  print(line['time'],line['name'])\n",
    "                    tim = line['time']\n",
    "\n",
    "                    spl = tim.split(\"T\")\n",
    "                    tim = spl[0]\n",
    "                    name = line['name'].strip()\n",
    "\n",
    "                    msg= line['msg'].strip()\n",
    "\n",
    "                    if \"err\" in line:\n",
    "                        err = line['err']\n",
    "                        msg+= f\"\\n\\n-------------\\n{err}\"\n",
    "\n",
    "\n",
    "                    uid = f\"{line['time']}{msg[1:20]}\"\n",
    "                    if uid in ids:\n",
    "                        ids[uid]+=1\n",
    "                    else:\n",
    "                        ids[uid]=1\n",
    "\n",
    "                    stitle=\"\"\n",
    "                    xm = msg.find(\"Full ETL failure for \")\n",
    "                    if xm > -1:\n",
    "                       stitle = msg[xm+20:].split(\":\")[0]\n",
    "                       stitle=stitle.replace(\" at load\",\"\")\n",
    "\n",
    "                    xm = msg.find(\"Error Loading \")\n",
    "                    if xm > -1:\n",
    "                       stitle = msg[xm+14:]\n",
    "\n",
    "                    xm = msg.find(\"Error Extracting \")\n",
    "                    if xm > -1:\n",
    "                       stitle=msg[xm+16:]\n",
    "\n",
    "                    xm = msg.find(\"Error Transforming \")\n",
    "                    if xm > -1:\n",
    "                       stitle=msg[xm+18:]\n",
    "\n",
    "                #   \n",
    "                    titl=\"\"\n",
    "                    s4x4=\"\"\n",
    "                    w4x4=\"\"\n",
    "                    try:\n",
    "                        s4x4s = re.findall(\"[\\w]{3,4}-[\\w]{3,4}\",line['msg'])\n",
    "\n",
    "\n",
    "                        if len(s4x4s) > 0:\n",
    "                          for s4x4 in s4x4s:\n",
    "                             if s4x4 in xrefsBy4x4:\n",
    "                                titl = xrefsBy4x4[s4x4]\n",
    "                                w4x4=s4x4\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        titl=\"\"\n",
    "\n",
    "\n",
    "                    if tim in errorsByDate:\n",
    "                        errorsByDate[tim]['name'].append(name)\n",
    "                        errorsByDate[tim]['msg'].append(msg)\n",
    "                        errorsByDate[tim]['line'].append(line)\n",
    "                        errorsByDate[tim]['file'].append(file)\n",
    "                        errorsByDate[tim]['title'].append(titl)\n",
    "                        errorsByDate[tim]['stitle'].append(stitle)\n",
    "                        errorsByDate[tim]['4x4'].append(w4x4)\n",
    "                        errorsByDate[tim]['uid'].append(uid)\n",
    "\n",
    "                    else:\n",
    "                        errorsByDate[tim] = {}\n",
    "                        errorsByDate[tim]['name'] = []\n",
    "                        errorsByDate[tim]['msg'] = []\n",
    "                        errorsByDate[tim]['line'] = []\n",
    "                        errorsByDate[tim]['file'] = []\n",
    "                        errorsByDate[tim]['title'] = []\n",
    "                        errorsByDate[tim]['stitle'] = []\n",
    "                        errorsByDate[tim]['4x4'] = []\n",
    "                        errorsByDate[tim]['uid'] = []\n",
    "\n",
    "                        errorsByDate[tim]['name'].append(name)\n",
    "                        errorsByDate[tim]['msg'].append(msg)\n",
    "                        errorsByDate[tim]['line'].append(line)\n",
    "                        errorsByDate[tim]['file'].append(file)\n",
    "                        errorsByDate[tim]['title'].append(titl)\n",
    "                        errorsByDate[tim]['stitle'].append(stitle)\n",
    "                        errorsByDate[tim]['4x4'].append(w4x4)\n",
    "                        errorsByDate[tim]['uid'].append(uid)\n",
    "\n",
    "\n",
    "\n",
    "                    if name in errorsByName:\n",
    "                            errorsByName[name]['time'].append(tim)\n",
    "                            errorsByName[name]['msg'].append(msg)\n",
    "                            errorsByName[name]['line'].append(line)\n",
    "                            errorsByName[name]['file'].append(file)\n",
    "                            errorsByName[name]['title'].append(titl)\n",
    "                    else:\n",
    "                        errorsByName[name] = {}\n",
    "                        errorsByName[name]['time'] = []\n",
    "                        errorsByName[name]['msg'] = []\n",
    "                        errorsByName[name]['line'] = []\n",
    "                        errorsByName[name]['file'] = []\n",
    "                        errorsByName[name]['title'] = []\n",
    "\n",
    "                        errorsByName[name]['time'].append(tim)\n",
    "                        errorsByName[name]['msg'].append(msg)\n",
    "                        errorsByName[name]['line'].append(line)\n",
    "                        errorsByName[name]['file'].append(file)\n",
    "                        errorsByName[name]['title'].append(titl)\n",
    "\n",
    "#                 except Exception as err:\n",
    "#                     print(err)\n",
    "#                     print(line)\n",
    "    return errorsByDate,errorsByName\n",
    "\n",
    "\n",
    "def getXrefs():\n",
    "    '''Reads the Inventory google sheet and gets the datasets title and cross-refs it to the Socrata 4x4 id.  Also\n",
    "    gets the fields by 4x4 dataset id and by the title'''\n",
    "    scope = ['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "             \"https://www.googleapis.com/auth/drive.file\",\n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('../client_secret.json',\n",
    "     scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    gc = gspread.service_account(\"../client_secret.json\")\n",
    "    # for gg in gc.list_spreadsheet_files():\n",
    "    #      print(\"GGGGG \",gg)\n",
    "    # https://docs.google.com/spreadsheets/d/1WTaOglzbSsYiHhAGguGxHQXmAGmOhfFHkGkMLowxAOA/edit?usp=sharing\n",
    "    sheet = client.open('BIC Data Inventory and Metadata').worksheet(\n",
    "        'Maintenance_Framework')\n",
    "    repo_sheet = client.open('BIC Data Inventory and Metadata').worksheet(\n",
    "        'MetadataRepository')\n",
    "    fields_sheet = client.open('BIC Data Inventory and Metadata').worksheet(\n",
    "        'Field Descriptions')\n",
    "    \n",
    "    \n",
    "#     sheet = client.open('https://docs.google.com/spreadsheets/d/1Xc7oYpdCLwHmUCaokpfXkF4bzkwRW0xUbvBZwruF0ZI/').worksheet(\n",
    "#         'Maintenance_Framework')\n",
    "#     repo_sheet = client.open('https://docs.google.com/spreadsheets/d/1Xc7oYpdCLwHmUCaokpfXkF4bzkwRW0xUbvBZwruF0ZI/').worksheet(\n",
    "#         'MetadataRepository')\n",
    "    \n",
    "#     fields_sheet = client.open('https://docs.google.com/spreadsheets/d/1Xc7oYpdCLwHmUCaokpfXkF4bzkwRW0xUbvBZwruF0ZI/').worksheet(\n",
    "#         'Field Descriptions')\n",
    "\n",
    "    dfRepo = pd.DataFrame(repo_sheet.get_all_records(head=3))\n",
    "    xrefsBy4x4 = {}\n",
    "    xrefsByTitle = {}\n",
    "\n",
    "    for index,row in dfRepo[['Dataset Title','Socrata Link']].iterrows():\n",
    "        xrefsByTitle[row['Dataset Title']] = row['Socrata Link']\n",
    "        xrefsBy4x4[row['Socrata Link']] = row['Dataset Title']\n",
    "        \n",
    "    dfFields = pd.DataFrame(fields_sheet.get_all_records(head=1))\n",
    "    fields = {}\n",
    "    for index,row in dfFields.iterrows():\n",
    "        s4x4 = row[\"Socrata ID\"]\n",
    "        of = row[\"Source Field Name\"]\n",
    "        tf = row[\"Full Field Name\"]\n",
    "        af = row[\"API Field Name\"]\n",
    "        if s4x4 in fields:\n",
    "            fields[s4x4][\"source\"].append(of)\n",
    "            fields[s4x4][\"cim\"].append(tf)\n",
    "            fields[s4x4][\"api\"].append(af)\n",
    "        else:\n",
    "            fields[s4x4] = {}\n",
    "            fields[s4x4][\"source\"] = []\n",
    "            fields[s4x4][\"cim\"] = []\n",
    "            fields[s4x4][\"api\"] = []\n",
    "            \n",
    "            fields[s4x4][\"source\"].append(of)\n",
    "            fields[s4x4][\"cim\"].append(tf)\n",
    "            fields[s4x4][\"api\"].append(af)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return xrefsBy4x4,xrefsByTitle,fields\n",
    "\n",
    "############################################################\n",
    "\n",
    "def toGoogleSheet(row):\n",
    "    '''Reads the Inventory google sheet and gets the datasets title and cross-refs it to the Socrata 4x4 id.  Also\n",
    "    gets the fields by 4x4 dataset id and by the title'''\n",
    "    # scope = ['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "    #          \"https://www.googleapis.com/auth/drive.file\",\n",
    "    #               \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "    path='../client_secret.json'\n",
    "    gc=pygsheets.authorize(service_account_file=path)\n",
    "    sh=gc.open('Changes/Fixes Requested by BIC')\n",
    "    wk1=sh[0]\n",
    "    ret = wk1.append_table(row)\n",
    "    return ret\n",
    "\n",
    "def markDone(row):\n",
    "    '''Reads the Inventory google sheet and gets the datasets title and cross-refs it to the Socrata 4x4 id.  Also\n",
    "    gets the fields by 4x4 dataset id and by the title'''\n",
    "    # scope = ['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "    #          \"https://www.googleapis.com/auth/drive.file\",\n",
    "    #               \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "    path='../client_secret.json'\n",
    "    gc=pygsheets.authorize(service_account_file=path)\n",
    "    sh=gc.open('ETL Errors Accounted For')\n",
    "    wk1=sh[0]\n",
    "    ret = wk1.append_table(row)\n",
    "    return ret\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def getErrorStatus():\n",
    "    '''Reads the Inventory google sheet and gets the datasets title and cross-refs it to the Socrata 4x4 id.  Also\n",
    "    gets the fields by 4x4 dataset id and by the title'''\n",
    "    # scope = ['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "    #          \"https://www.googleapis.com/auth/drive.file\",\n",
    "    #               \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "    path='../client_secret.json'\n",
    "    gc=pygsheets.authorize(service_account_file=path)\n",
    "    sh=gc.open('ETL Errors Accounted For')\n",
    "    wk1=sh[0]\n",
    "    df = wk1.get_as_df()\n",
    "    x=df[[\"Unique ID\",\"Status\"]].values.tolist()\n",
    "    \n",
    "    return {y[0]:y[1] for y in x}\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def getLogSummary():\n",
    "    '''Reads the Inventory google sheet and gets the datasets title and cross-refs it to the Socrata 4x4 id.  Also\n",
    "    gets the fields by 4x4 dataset id and by the title'''\n",
    "    # scope = ['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "    #          \"https://www.googleapis.com/auth/drive.file\",\n",
    "    #               \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "    path='../client_secret.json'\n",
    "    gc=pygsheets.authorize(service_account_file=path)\n",
    "    sh=gc.open('Changes/Fixes Requested by BIC')\n",
    "    wk1=sh[0]\n",
    "    df = wk1.get_as_df()\n",
    "    x=df.values.tolist()\n",
    "    \n",
    "    return x,list(df.columns)\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def showError(row):\n",
    "    layout = [\n",
    "        [sg.Button(\"Quit\")],\n",
    "        [sg.Button(\"Write Log\")],\n",
    "        [sg.Button(\"Mark Done\"),sg.Button(\"Mark Skip\")],\n",
    "        [sg.Multiline(\"\",s=(50,10),key=\"-ERRORSINGLE-\",font=\"CENTAUR 15 bold\")],\n",
    "        [sg.Text(\"Title: \",font=\"CENTAUR 15 bold\"),\n",
    "         sg.Multiline(\"\",s=(50,2),key=\"-ERRORTITLE-\",font=\"CENTAUR 15 bold\",text_color=\"black\")],\n",
    "         [sg.Multiline(\"JIRA BIC-\",s=(10,2),key=\"-ERRORJIRA-\",font=\"CENTAUR 15 bold\")],\n",
    "        [sg.Text(\"NOTES: \",font=\"CENTAUR 15 bold\"),\n",
    "         sg.Multiline(\"\",s=(20,10),key=\"-ERRORNOTES-\",font=\"CENTAUR 15 bold\")]\n",
    "        \n",
    "    ]\n",
    "    window = sg.Window('Show Error', layout, finalize=True,metadata=row)\n",
    "    window[\"-ERRORSINGLE-\"].print(f\"Date: {row[0]}\", text_color='black')\n",
    "    window[\"-ERRORSINGLE-\"].print(f\"Dataset: {row[1]}\", text_color='black')\n",
    "    window[\"-ERRORSINGLE-\"].print(f\"4x4: {row[2]}\", text_color='black')\n",
    "    window[\"-ERRORSINGLE-\"].print(f\"Title: {row[3]}\", text_color='black')\n",
    "    window[\"-ERRORSINGLE-\"].print(f\"Stitle: {row[5]}\", text_color='black')\n",
    "    \n",
    "    window[\"-ERRORSINGLE-\"].print(f\"\\nMessage:\\n{row[4]}\", text_color='red')\n",
    "    if len(row[3]) > 10:\n",
    "       window[\"-ERRORTITLE-\"].print(f\"{row[3]}\", text_color='black')\n",
    "    elif len(row[5]) > 10:\n",
    "       window[\"-ERRORTITLE-\"].print(f\"{row[5]}\", text_color='black')\n",
    "        \n",
    "##############################################################################       \n",
    "                                  \n",
    "def showRecentLogs(errorsByDate):\n",
    "    xx = []\n",
    "    colWidths = (10,10,10,80)\n",
    "    rowColors = []\n",
    "    count=0\n",
    "    date0=list(errorsByDate.keys())[0]\n",
    "    cols=(\"plum4\")\n",
    "    bg=\"white\"\n",
    "    bgs=[]\n",
    "    count2=0\n",
    "    order = {}\n",
    "    for nn in errorsByDate.keys():\n",
    "        order[nn]= min(errorsByDate[nn][\"time\"])\n",
    "    \n",
    "    order = sorted(order.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    errorStatus = getErrorStatus()\n",
    " #   for date in sorted(errorsByDate,reverse=True):\n",
    "    for mrec in order:\n",
    "        date = mrec[0]\n",
    "        \n",
    "        string=f\"Date: {date};;\\n\\n\"\n",
    "        msg=\"\"\n",
    "        for nn in range(len(errorsByDate[date][\"name\"])):\n",
    "    #        print(f'    {errorsByDate[date][\"name\"][nn]}\\n      M {errorsByDate[date][\"msg\"][nn]}\\n      F{errorsByDate[date][\"file\"][nn]}\\n      L {errorsByDate[date][\"line\"][nn]}\\n')\n",
    "        \n",
    "          #  msg = textwrap.fill(errorsByDate[date][\"msg\"][nn],75)\n",
    "            if nn > 0:\n",
    "                msg+=\"\\n-----------------------------------\\n\"\n",
    "            msg+= errorsByDate[date][\"msg\"][nn]\n",
    "            uid = errorsByDate[date][\"uid\"][nn]\n",
    "            if uid not in errorStatus:\n",
    "           ##     yy = [date,errorsByDate[date][\"name\"][nn],errorsByDate[date][\"4x4\"][nn],errorsByDate[date][\"title\"][nn],msg,errorsByDate[date][\"stitle\"][nn],errorsByDate[date][\"uid\"][nn]]\n",
    "           ##     xx.append(yy)\n",
    "                if date != date0:\n",
    "                    count+=1\n",
    "                    if count%2 == 0:\n",
    "                        cols = (\"plum4\")\n",
    "                        bg=\"white\"\n",
    "                    else:\n",
    "                        cols = (\"SteelBlue3\")\n",
    "                        bg=\"black\"\n",
    "                if count2%2 == 0:\n",
    "                    cols=\"plum4\"\n",
    "                else:\n",
    "                    cols=\"SteelBlue3\"\n",
    "                count2+=1\n",
    "                date0=date\n",
    "                rowColors.append(cols)\n",
    "                bgs.append(bg)\n",
    "        yy = [errorsByDate[date][\"time\"][0],errorsByDate[date][\"name\"][0],errorsByDate[date][\"4x4\"][0],errorsByDate[date][\"title\"][0],msg,errorsByDate[date][\"stitle\"][nn],errorsByDate[date][\"uid\"][0]]\n",
    "        xx.append(yy)\n",
    "\n",
    "    rowNums = [num for num in range(0,len(rowColors)+1)]\n",
    "    colrw = list(zip(rowNums,rowColors))\n",
    "    colrw=list(zip(rowNums,bgs,rowColors))\n",
    "   \n",
    " #   print(\"XX XX XX\",xx)\n",
    "\n",
    "    header = [\"Date\",\"Dataset\",\"4x4\",\"Title\",\"Message\",\"Title Guess\",\"UID\"]\n",
    "\n",
    "    layout = [\n",
    "        [sg.Button(\"Quit\",font=\"CENTAUR 15 bold\")],\n",
    "        [sg.Button(\"Include Completed\",visible=True,font=\"CENTAUR 15 bold\"),\n",
    "         sg.Button(\"Hide Completed\",visible=False,font=\"CENTAUR 15 bold\")],\n",
    "        [sg.Text(\"Errors for the Last 2 Months\",font=\"CENTAUR 15 bold\")],\n",
    "        [sg.Table(values=xx,headings=header,visible_column_map=[True,True,True,True,True,False,False], size=(120, 30),row_height=40,row_colors=colrw,vertical_scroll_only=False,max_col_width=60,enable_events=True, key='-DAILY-',col_widths=colWidths)],\n",
    "        [sg.Push(), sg.Button('Update')],\n",
    "    ]\n",
    "    window = sg.Window('Title', layout, finalize=True,metadata=xx)\n",
    "    table = window[\"-DAILY-\"]  \n",
    "    table.bind('<Button-1>', \"Click\")\n",
    "    window[\"-DAILY-\"].Widget.column('#4', anchor='w') \n",
    "    return window\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def updateLogs(how=1):\n",
    "    #  how    1 = update just this month (log.json.nn\n",
    "    #         2 = update Last Months file (log_backup_year_mo.json\n",
    "    #         3 = update all this years log files \n",
    "    pw=os.environ[\"OIT_PW\"]\n",
    "    #  compute previous month\n",
    "    x = datetime.today() - timedelta(days=30)\n",
    "    monthM = x.month  \n",
    "    yearM = x.year\n",
    "    print(\"Updating ETL Error Logs\", flush=True)\n",
    "    year = datetime.today().year\n",
    "    \n",
    "    string = f'''sshpass -p {pw} sftp -o HostKeyAlgorithms=+ssh-rsa -o PubkeyAcceptedAlgorithms=+ssh-rsa  giddensm@165.127.62.8 << !'''\n",
    "\n",
    "    if how == 1:  #  update this month\n",
    "        string+= f\"\\nmget /usr/local/cim/bic_etl/general/logs/log.json* {path}\"\n",
    "##  remove all of this months logs first to account for month changes\n",
    "        print(\"\\nCleaning Up Old FIles for THIS Month\\n\", flush=True)\n",
    "        for x in os.listdir(path):\n",
    "            if  re.findall(\"^log.json\\.\\d+\",x):\n",
    "                 file=f\"{path}/{x}\"\n",
    "                 print(\"Removing Local Log File\",file, flush=True)\n",
    "                 os.remove(file)\n",
    "        file=f\"{path}/log.json\"\n",
    "        print(\"Removing Local Log File\",file, flush=True)\n",
    "        os.remove(file)\n",
    "        \n",
    "    elif how == 2: \n",
    "        string+=f\"\\nmget /usr/local/cim/bic_etl/general/logs/log_backup_{yearM}_{monthM}*.json {path}\"\n",
    "    elif how == 3:\n",
    "        string+=f\"\\nmget /usr/local/cim/bic_etl/general/logs/log_backup_{year}*.json {path}\"\n",
    "    \n",
    "    string+=\"\\n!'''\"\n",
    "\n",
    " #   print(string)\n",
    "    result=subprocess.run([string], shell=True, stdout=subprocess.PIPE)\n",
    "    x = str(result.stdout)\n",
    "    nfiles=0\n",
    "    files=x.split(\"\\\\n\")\n",
    "    filesOut=[]\n",
    "    for line in files:\n",
    "     #   print(line)\n",
    "        if \"fetch\" in line.lower():\n",
    "            nfiles+=1\n",
    "            filesOut.append(line)      \n",
    "            \n",
    "    return nfiles,filesOut\n",
    "    \n",
    "##################################################\n",
    "\n",
    "def orderErrors(errorsByDate,errorStatus,flag):\n",
    "    xx=[]\n",
    "    bgs=[]\n",
    "    rowColors=[]\n",
    "    date0=list(errorsByDate.keys())[0]\n",
    "    count=0\n",
    "    count2=0\n",
    "    cols=(\"plum4\")\n",
    "    bg=\"white\"\n",
    "    for date in sorted(errorsByDate,reverse=True):\n",
    "        string=f\"Date: {date};;\\n\\n\"\n",
    "        for nn in range(len(errorsByDate[date][\"name\"])):\n",
    "            msg = errorsByDate[date][\"msg\"][nn]\n",
    "            uid = errorsByDate[date][\"uid\"][nn]\n",
    "            if uid not in errorStatus or flag==1:\n",
    "                yy = [date,errorsByDate[date][\"name\"][nn],errorsByDate[date][\"4x4\"][nn],errorsByDate[date][\"title\"][nn],msg,errorsByDate[date][\"stitle\"][nn],errorsByDate[date][\"uid\"][nn]]\n",
    "                xx.append(yy)\n",
    "                if date != date0:\n",
    "                    count+=1\n",
    "                    if count%2 == 0:\n",
    "                        cols = (\"plum4\")\n",
    "                        bg=\"white\"\n",
    "                    else:\n",
    "                        cols = (\"SteelBlue3\")\n",
    "                        bg=\"black\"\n",
    "                if count2%2 == 0:\n",
    "                    cols=\"plum4\"\n",
    "                else:\n",
    "                    cols=\"SteelBlue3\"\n",
    "                count2+=1\n",
    "                date0=date\n",
    "                if uid in errorStatus:\n",
    "                    cols=\"grey\"\n",
    "                rowColors.append(cols)\n",
    "                bgs.append(bg)\n",
    "\n",
    "\n",
    "    rowNums = [num for num in range(0,len(rowColors)+1)]\n",
    "  # colrw = list(zip(rowNums,rowColors))\n",
    "    colrw=list(zip(rowNums,bgs,rowColors))\n",
    "    return xx,colrw\n",
    "   \n",
    "#####################################################################\n",
    "\n",
    "def showLogSummary(values,columns):\n",
    "    layout = [[sg.Text(f\"Log Summary\",font=\"CENTAUR 15 bold\")],\n",
    "              [sg.Button(\"Quit\")],\n",
    "               [sg.Table(values=values,text_color=\"black\", auto_size_columns=False,enable_events=True,num_rows=10,font=\"CENTAUR 10\",\n",
    "                   justification='left',vertical_scroll_only=False,background_color=\"#F8C471\",\n",
    "                   alternating_row_color=\"#FAE5D3\",key='-LOGSUMMARY-',headings = columns)]\n",
    "               ]\n",
    "    \n",
    "\n",
    "#    sg.theme(colorTheme)     \n",
    "    window2 = sg.Window(\"Log Summary\",layout,finalize=True,resizable=True)\n",
    "    a = window2.CurrentLocation()\n",
    "    screen_width, screen_height = window2.get_screen_dimensions()\n",
    "    win_width, win_height = window2.size\n",
    "    x, y = (screen_width - win_width)//2, (screen_height - win_height)//2\n",
    "    x=200\n",
    "    y=200\n",
    "    window2.move(x, y)\n",
    "    tableFile1 = window2['-LOGSUMMARY-']\n",
    "    tableFile1.bind('<Button-1>', \"Click\")\n",
    "   # headerStore[tableFile1] = columns\n",
    "   # statsStore[tableFile1] = stats1\n",
    "   \n",
    "    return window2\n",
    "\n",
    "\n",
    "\n",
    "xrefsBy4x4,xrefsByTitle,fields = getXrefs()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28786f69-106b-4943-9864-8aa272e7f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getLogSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea661f2d-d94b-44a2-8ab6-4689e074aa56",
   "metadata": {},
   "source": [
    "## GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5247b38-4a96-46a3-8945-a8fd5b731a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# import OpenGL\n",
    "# from OpenGL import GLU\n",
    "\n",
    "\n",
    "##########################################################\n",
    "\n",
    "def compare2Data(df,xrefs):\n",
    "    global dcols,pcols,rowColors,values,dnotpLines\n",
    "    dcols = list(df.columns)\n",
    "    pcols = list(xrefs.keys())\n",
    "    \n",
    "    wid = windowsStore[\"columnAnalysis\"]\n",
    "    rowColors = wid[\"-TABLECOLUMN-\"].metadata[1]\n",
    "    values = wid[\"-TABLECOLUMN-\"].metadata[0]\n",
    "   \n",
    "    colr1 = rowColors[0][1]\n",
    "    colr2 = rowColors[1][1]\n",
    "    \n",
    "    pnotd = []\n",
    "    dnotp = []\n",
    "    dnotpLines = {}\n",
    "\n",
    "    for col in pcols:\n",
    "        if col not in dcols:\n",
    "            pnotd.append(col)\n",
    "\n",
    "    for col in dcols:\n",
    "        if col not in pcols:\n",
    "            dnotp.append(col)\n",
    "            for line in transformProgram:\n",
    "                if line.find(col) > -1:\n",
    "                   if col in dnotpLines:\n",
    "                     dnotpLines[col].append(line)\n",
    "                   else:\n",
    "                     dnotpLines[col] = []\n",
    "                     dnotpLines[col].append(line)\n",
    "                                 \n",
    "    for nn,value in enumerate(values):    \n",
    "        if nn%2 == 0:\n",
    "            rowColor= colr1\n",
    "        else:\n",
    "            rowColor= colr2\n",
    "\n",
    "        col = value[0]\n",
    "\n",
    "        if col in pnotd:\n",
    "            rowColor = \"pink\"\n",
    "\n",
    "        colr = (nn,rowColor)\n",
    "        rowColors.append(colr)\n",
    "\n",
    "        \n",
    "    sp = \"\\n\".join(pnotd)\n",
    "    sd = \"\\n\".join(dnotp)\n",
    "    \n",
    "        \n",
    "    wid[\"-TABLECOLUMN-\"].update(row_colors=rowColors)  \n",
    "    wid[\"-PNOTD-\"].update(pnotd)    \n",
    "    wid[\"-DNOTP-\"].update(dnotp)    \n",
    "    \n",
    "    \n",
    "##########################################################\n",
    "\n",
    "def compareWindow(stats1,df1,file1,title=\"Compare\",colorTheme=\"'Dark Green 5'\"):\n",
    "    global color1,color2\n",
    "    header_list = [\"Column\",\"% Missing\",\"Missing\",\"string\",\"integer\",\"float\",\"boolean\"]\n",
    "  \n",
    "    col_widths = [8]*len(header_list)\n",
    "    col_widths[0] = 25\n",
    "    columnSortStateTable1 = {}\n",
    "  \n",
    "\n",
    "    ## set up sort state for the column in both tables\n",
    "    for col in header_list:\n",
    "         columnSortStateTable1[col] = -1\n",
    "       \n",
    "            \n",
    "    valsFile1 = getValues(stats1,header_list)\n",
    "   \n",
    "    \n",
    "    rowFile1Colors = setRowColors(valsFile1,color1,color2,\"pink\",header_list)\n",
    "    \n",
    "    layCol1 = [[sg.Text(f\"File 1 {file1}\",font=\"CENTAUR 15\")],[sg.Text(f\"File 1 Shape {df1.shape}\",font=\"CENTAUR 15\")],\n",
    "               [sg.Button(\"Output Columns\")],\n",
    "               [sg.Table(values=valsFile1,text_color=\"black\", auto_size_columns=False,enable_events=True,num_rows=20,col_widths=col_widths,font=\"CENTAUR 10\",\n",
    "                   justification='center',pad=(5,5),vertical_scroll_only=False,\n",
    "                   key='-TABLEFILE-',row_colors=rowFile1Colors,headings = header_list,metadata=columnSortStateTable1)]\n",
    "               ]\n",
    "    \n",
    "\n",
    "    layout = [[sg.Button(\"Quit\")],\n",
    "    layCol1]  \n",
    "              \n",
    "    sg.theme(colorTheme)     \n",
    "    window2 = sg.Window(title,layout,finalize=True,resizable=True,metadata=[colorTheme,df1,title])\n",
    "    a = window2.CurrentLocation()\n",
    "    screen_width, screen_height = window2.get_screen_dimensions()\n",
    "    win_width, win_height = window2.size\n",
    "    x, y = (screen_width - win_width)//2, (screen_height - win_height)//2\n",
    "    x=200\n",
    "    y=200\n",
    "    window2.move(x, y)\n",
    "    tableFile1 = window2['-TABLEFILE-']\n",
    "    tableFile1.bind('<Button-1>', \"Click\")\n",
    "    headerStore[tableFile1] = header_list\n",
    "    statsStore[tableFile1] = stats1\n",
    "#    print(\"STATS \",stats1)\n",
    "    return window2,tableFile1,valsFile1,rowFile1Colors\n",
    "\n",
    "#############################################\n",
    "\n",
    "def dfAnalyze(df):\n",
    "    stats = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        typs = df[col].apply(type).value_counts().to_dict()\n",
    "        stats[col] = {}\n",
    "        if str in typs:\n",
    "           stats[col][\"string\"] = typs[str]\n",
    "        else:\n",
    "           stats[col][\"string\"] = 0\n",
    "\n",
    "        if int in typs:\n",
    "           stats[col][\"integer\"] = typs[int]\n",
    "        else:\n",
    "           stats[col][\"integer\"] = 0\n",
    "\n",
    "        if float in typs:\n",
    "           stats[col][\"float\"] = typs[float]\n",
    "        else:\n",
    "           stats[col][\"float\"] = 0\n",
    "\n",
    "        if bool in typs:\n",
    "           stats[col][\"boolean\"] = typs[bool]\n",
    "        else:\n",
    "           stats[col][\"boolean\"] = 0\n",
    "\n",
    "        stats[col][\"Missing\"] = df[col].isna().sum()\n",
    "        stats[col][\"% Missing\"] = round(df[col].isna().sum()/df.shape[0]*100,1)\n",
    "        \n",
    "    return stats\n",
    "\n",
    "############################################# \n",
    "\n",
    "def exceptionLog(exception,funCall):\n",
    "  exception_message = str(exception)\n",
    "  exception_type, exception_object, exception_traceback = sys.exc_info()\n",
    "  filename = os.path.split(exception_traceback.tb_frame.f_code.co_filename)[1]\n",
    "  print(f\"{exception_message} {exception_type} {funCall}, Line {exception_traceback.tb_lineno}\")\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def fullDataColumnMap(cim4x4):\n",
    "    '''Map the data fields found in the source data file to all the other sources  '''\n",
    "    global missed\n",
    "    if cim4x4 in fields:\n",
    "        invc = fields[cim4x4][\"source\"].copy()\n",
    "    else:\n",
    "        invc = []\n",
    "    tfc = transformColumns.copy()\n",
    "    exc = extractColumns.copy()\n",
    "    excMissed = []\n",
    "    tfcsij = transformSijColumns.copy()\n",
    "    loadsij = loadSijColumns.copy()\n",
    "    cim = cimColumns.copy()\n",
    "    xrfo2t = xrefsO2T.copy()\n",
    "    names = []\n",
    "    columnDict = {}\n",
    "    names.append(\"BIC Inventory\")\n",
    "    names.append(\"Transform Program\")\n",
    "\n",
    "    names.append(\"Transform - Data\")\n",
    "    names.append(\"CIM - Data\")\n",
    "    names.append(\"Transform - SIJ\")\n",
    "    names.append(\"Load - SIJ\")\n",
    "\n",
    "    for k in exc:\n",
    "       # print(\"MAP \",k)\n",
    "    #for k,v in xrefsO2T.items():\n",
    "        columnDict[k] = []\n",
    "        nhit=0\n",
    "        v=\"\"\n",
    "        if k in invc:\n",
    "            v=k\n",
    "            vv=k\n",
    "            invc.remove(k)\n",
    "            nhit+=1\n",
    "        else:\n",
    "            vv=\"\"\n",
    "        columnDict[k].append(vv)\n",
    "        \n",
    "        if k in xrfo2t:\n",
    "            v = xrfo2t[k]\n",
    "            vv=v\n",
    "            del xrfo2t[k]\n",
    "            nhit+=1\n",
    "        else:\n",
    "            vv = \"\"\n",
    "        columnDict[k].append(vv)  \n",
    "\n",
    "   \n",
    "        if v in tfc:\n",
    "           tfc.remove(v)\n",
    "           cc = v\n",
    "           nhit+=1\n",
    "        else:\n",
    "            cc=\"\"\n",
    "        columnDict[k].append(cc)\n",
    "\n",
    "\n",
    "        if v in cim:\n",
    "           cim.remove(v)\n",
    "           cc = v\n",
    "           nhit+=1            \n",
    "        else:\n",
    "            cc=\"\"\n",
    "        columnDict[k].append(cc)\n",
    "\n",
    "\n",
    "\n",
    "        if v.lower() in tfcsij:\n",
    "           tfcsij.remove(v.lower())\n",
    "           cc = v.lower()\n",
    "           nhit+=1            \n",
    "        else:\n",
    "            cc=\"\"\n",
    "        columnDict[k].append(cc)\n",
    "\n",
    "        if v.lower() in loadsij:\n",
    "           loadsij.remove(v.lower())\n",
    "           cc = v.lower()\n",
    "           nhit+=1\n",
    "        else:\n",
    "            cc=\"\"\n",
    "        columnDict[k].append(cc)\n",
    "        \n",
    "        if nhit == 0:\n",
    "            excMissed.append(k)\n",
    "        \n",
    "\n",
    "    a = pd.DataFrame(columnDict).T\n",
    "    a.columns = names\n",
    "    a.reset_index(inplace=True)\n",
    "    values = a.values.tolist()\n",
    "    cols = list(a.columns)\n",
    "   \n",
    "    missed = {}\n",
    "    missed[\"Extract - Data\"] = excMissed\n",
    "    missed[\"BIC Inventory\"] = invc\n",
    "    missed[\"Transform Program\"] = list(xrfo2t.keys())\n",
    "    missed[\"Transform - Data\"] = tfc\n",
    "    missed[\"CIM - Data\"] = cim\n",
    "    missed[\"Transform - SIJ\"] = tfcsij\n",
    "    missed[\"Load - SIJ\"] = loadsij\n",
    "    \n",
    "    \n",
    " #   missed = [invc,list(xrfo2t.keys()),tfc,cim,tfcsij,loadsij]\n",
    "    return cols,values,missed\n",
    "        \n",
    "\n",
    "###############################################################\n",
    "def getFilesClicked(file,window):\n",
    "#         if len(values[\"-FILE1-\"]) > 0:\n",
    "#             file = values[\"-FILE1-\"]\n",
    "#         elif len(values[\"-WEB1-\"]) > 0:\n",
    "#             file = values[\"-WEB1-\"]\n",
    "      \n",
    "        df = getFile(\"Local\",file,window)\n",
    "        stats = dfAnalyze(df)\n",
    "\n",
    "        return df,stats\n",
    "\n",
    "####################################################################\n",
    "       \n",
    "def getFile(how,file,WindowP):\n",
    "    \n",
    "    if how == \"Local\":\n",
    "        if file[-3:].lower() == \"tsv\":\n",
    "            delim = \"\\t\"\n",
    "            df=pd.read_csv(file,encoding=\"latin\",delimiter=delim)\n",
    "        elif file[-4:].lower() == \"xlsx\":\n",
    "            df=pd.read_excel(file,engine=\"openpyxl\")\n",
    "        else:\n",
    "            try:\n",
    "                df=pd.read_csv(file)\n",
    "            except Exception as err:\n",
    "                print(\"Error, trying with encoding=latin\")\n",
    "                df=pd.read_csv(file,encoding=\"latin\")\n",
    "    elif how == \"Fetch\":\n",
    "      \n",
    "  #      file=values[\"-WEB-\"]\n",
    "      #  getPrevFiles(2,file)\n",
    "\n",
    "  #      windowP[\"-PINFO-\"].update(f\"START reading WEB File:{file}:\")\n",
    "        df=pd.read_csv(file)\n",
    "      \n",
    "  #      windowP[\"-PINFO-\"].update(f\"FINISHED reading WEB File:{file}:\")\n",
    "   \n",
    "    stats = dfAnalyze(df)\n",
    "        \n",
    "    return df,stats\n",
    "\n",
    "############################################################\n",
    "\n",
    "def getRowClicked(table,columns):\n",
    "    col=\"\"\n",
    "    e = table.user_bind_event \n",
    "    region = table.Widget.identify('region', e.x, e.y)\n",
    "    if region == 'heading':\n",
    "        row = 0\n",
    "    elif region == 'cell':\n",
    "        row = int(table.Widget.identify_row(e.y))  \n",
    "        col = columns[row-1][0]\n",
    "        \n",
    "    return row,col    \n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def getRowClickedUN(table):\n",
    "    val=\"\"\n",
    "    data = dataStore[table]\n",
    "    e = table.user_bind_event \n",
    "    region = table.Widget.identify('region', e.x, e.y)\n",
    "    if region == 'heading':\n",
    "        row = 0\n",
    "    elif region == 'cell':\n",
    "        row = int(table.Widget.identify_row(e.y))  \n",
    "        val = data[row-1][0]\n",
    "      #  print(\"Un VAL CLICKED \",val)\n",
    "    return row,val    \n",
    "\n",
    "##########################################################\n",
    "\n",
    "def getSijColumns(file):\n",
    "    '''Get the fields for both teh contrile file content and the ftp control file content '''\n",
    "    global transformSijColumns,loadSijColumns\n",
    "    with open(\"/home/joe/bic_etl/cdos/business/nonprofit/scripts/datasync/char_paid_solicitors.sij\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        line = lines[0]\n",
    "        ss = json.loads(line)\n",
    "        cfc = json.loads(ss[\"controlFileContent\"])\n",
    "        transformSijColumns = cfc[\"csv\"][\"columns\"]\n",
    "        ftp = json.loads(ss[\"ftpControlFileContent\"])\n",
    "        loadSijColumns = ftp['csv']['columns']\n",
    "\n",
    "#         for col in transformColumns:\n",
    "#             if col in loadColumns:\n",
    "#                 hit+=1\n",
    "#             else:\n",
    "#                 miss+=1\n",
    "\n",
    "#         for col in loadColumns:\n",
    "#             if col in transformColumns:\n",
    "#                 hit+=1\n",
    "#             else:\n",
    "#                 miss+=1\n",
    "\n",
    "# ##########################################################\n",
    "\n",
    "def getValues(stats,header):\n",
    "\n",
    "    statsVals=[]\n",
    "    for col in sorted(stats.keys()):\n",
    "         vals=[]\n",
    "         vals.append(col)\n",
    "         for k in header[1:]:\n",
    "            vals.append(stats[col][k.strip()])\n",
    "         statsVals.append(vals)\n",
    "    return statsVals     \n",
    "\n",
    "##############################################################\n",
    "\n",
    "def getXrefs():\n",
    "    '''Reads the Inventory google sheet and gets the datasets title and cross-refs it to the Socrata 4x4 id.  Also\n",
    "    gets the fields by 4x4 dataset id and by the title'''\n",
    "    scope = ['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "             \"https://www.googleapis.com/auth/drive.file\",\n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('../client_secret.json',\n",
    "     scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    gc = gspread.service_account(\"../client_secret.json\")\n",
    "    # for gg in gc.list_spreadsheet_files():\n",
    "    #      print(\"GGGGG \",gg)\n",
    "    # https://docs.google.com/spreadsheets/d/1WTaOglzbSsYiHhAGguGxHQXmAGmOhfFHkGkMLowxAOA/edit?usp=sharing\n",
    "    sheet = client.open('BIC Data Inventory and Metadata').worksheet(\n",
    "        'Maintenance_Framework')\n",
    "    repo_sheet = client.open('BIC Data Inventory and Metadata').worksheet(\n",
    "        'MetadataRepository')\n",
    "    fields_sheet = client.open('BIC Data Inventory and Metadata').worksheet(\n",
    "        'Field Descriptions')\n",
    "    \n",
    "    \n",
    "#     sheet = client.open('https://docs.google.com/spreadsheets/d/1Xc7oYpdCLwHmUCaokpfXkF4bzkwRW0xUbvBZwruF0ZI/').worksheet(\n",
    "#         'Maintenance_Framework')\n",
    "#     repo_sheet = client.open('https://docs.google.com/spreadsheets/d/1Xc7oYpdCLwHmUCaokpfXkF4bzkwRW0xUbvBZwruF0ZI/').worksheet(\n",
    "#         'MetadataRepository')\n",
    "    \n",
    "#     fields_sheet = client.open('https://docs.google.com/spreadsheets/d/1Xc7oYpdCLwHmUCaokpfXkF4bzkwRW0xUbvBZwruF0ZI/').worksheet(\n",
    "#         'Field Descriptions')\n",
    "\n",
    "    dfRepo = pd.DataFrame(repo_sheet.get_all_records(head=3))\n",
    "    xrefsBy4x4 = {}\n",
    "    xrefsByTitle = {}\n",
    "\n",
    "    for index,row in dfRepo[['Dataset Title','Socrata Link']].iterrows():\n",
    "        xrefsByTitle[row['Dataset Title']] = row['Socrata Link']\n",
    "        xrefsBy4x4[row['Socrata Link']] = row['Dataset Title']\n",
    "        \n",
    "    dfFields = pd.DataFrame(fields_sheet.get_all_records(head=1))\n",
    "    fields = {}\n",
    "    for index,row in dfFields.iterrows():\n",
    "        s4x4 = row[\"Socrata ID\"]\n",
    "        of = row[\"Source Field Name\"]\n",
    "        tf = row[\"Full Field Name\"]\n",
    "        af = row[\"API Field Name\"]\n",
    "        if s4x4 in fields:\n",
    "            fields[s4x4][\"source\"].append(of)\n",
    "            fields[s4x4][\"cim\"].append(tf)\n",
    "            fields[s4x4][\"api\"].append(af)\n",
    "        else:\n",
    "            fields[s4x4] = {}\n",
    "            fields[s4x4][\"source\"] = []\n",
    "            fields[s4x4][\"cim\"] = []\n",
    "            fields[s4x4][\"api\"] = []\n",
    "            \n",
    "            fields[s4x4][\"source\"].append(of)\n",
    "            fields[s4x4][\"cim\"].append(tf)\n",
    "            fields[s4x4][\"api\"].append(af)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return xrefsBy4x4,xrefsByTitle,fields\n",
    "\n",
    "#############################################\n",
    "\n",
    "def go_deeper(aDict,value,nit):\n",
    "    nit+=1\n",
    "    for k, v in aDict.items():\n",
    "         if not bool(v):            \n",
    "             aDict[k] = []\n",
    "             aDict[k].append(value)\n",
    "         elif isinstance(v,list):\n",
    "             aDict[k].append(value)\n",
    "         else:\n",
    "             go_deeper(v,value,nit)\n",
    "   \n",
    "    return aDict\n",
    "\n",
    "###################################################    \n",
    "\n",
    "def init():\n",
    "    global dataSetsEtl,datasets,groupMenu\n",
    "    groups = []\n",
    "    desktop = pathlib.Path(\"/home/joe/bic_etl\")\n",
    "    runEtls = []\n",
    "    dataSets = []\n",
    "    info = {}\n",
    "    # .rglob() produces a generator too\n",
    "    desktop.rglob(\"*\")\n",
    "    files = list(desktop.rglob(\"*\"))\n",
    "# Which you can wrap in a list() constructor to materialize\n",
    "    for ff in files:\n",
    "\n",
    "            if (str(ff).split(\"/\")[-1] == \"run_etl.json\"):     \n",
    "                a=str(ff).split(\"/\")\n",
    "                m = a.index(\"bic_etl\")\n",
    "                b = a[m+1:-1]\n",
    "                ll = splitL(b)\n",
    "                groups.append(ll)\n",
    "                runEtls.append(ff)\n",
    "            \n",
    " #   print(f\"{len(runEtls)} run_etl.json files found\")    \n",
    "    \n",
    "    dataSets = []\n",
    "    groups = []\n",
    "    for file in runEtls:\n",
    "      f = open(file,\"r\")\n",
    "      data = json.load(f)\n",
    "      # print(file)\n",
    "      # print(\"----\")\n",
    "      a=str(file).split(\"/\")\n",
    "      m = a.index(\"bic_etl\")\n",
    "      b = a[m+1:-1]\n",
    "      group = b[0]\n",
    "      ll = splitL(b)\n",
    "      bdir = \"/\".join(b)  \n",
    "    \n",
    "      \n",
    "    #  groups.append(ll)\n",
    "      for val in data:\n",
    "            if \"title\" in val:\n",
    "                  title=val[\"title\"]\n",
    "                  info[title]={}\n",
    "                  info[title][\"directory\"] = bdir\n",
    "                  info[title][\"group\"] = group\n",
    "                \n",
    "    #              print(title,ll)\n",
    "                  nit=0\n",
    "                  ll = go_deeper(ll,title,nit)\n",
    "             #     info[title][\"groups\"] = ll\n",
    "            \n",
    "     #             print(ll)\n",
    "     #             groups.append(ll)\n",
    "            dataSets.append(val)\n",
    "    #  print(\"FF \",ll) \n",
    "      groups.append(ll)\n",
    "\n",
    "    datasets = []\n",
    "    dataSetsEtl={}\n",
    "    for val in dataSets:\n",
    "        if \"title\" in val:\n",
    "          datasets.append(val[\"title\"])\n",
    "          title=val[\"title\"]\n",
    "          dataSetsEtl[title] = {}  \n",
    "          dataSetsEtl[title][\"info\"] = {}\n",
    "          dataSetsEtl[title][\"info\"][\"directory\"] = info[title][\"directory\"]\n",
    "          dataSetsEtl[title][\"info\"][\"group\"] = info[title][\"group\"]\n",
    "     #     dataSetsEtl[title][\"info\"][\"groups\"] = info[title][\"groups\"]\n",
    "            \n",
    "        \n",
    "            \n",
    "          for k,v in val.items():\n",
    "          #      print(k,v)\n",
    "                if k != \"title\":\n",
    "                    dataSetsEtl[title][k] = {}\n",
    "                    if isinstance(v,dict):\n",
    "                        for k1,v1 in v.items():\n",
    "                            dataSetsEtl[title][k][k1]=v1\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "    groupMenu = ['Groups',\n",
    "     ['boulder',\n",
    "      ['Restaurant Inspections in Boulder Colorado'],\n",
    "      'catalog',\n",
    "      ['CIM Catalog Download'],\n",
    "      'cdhe',\n",
    "      ['Enrollment Demographics for Post-Secondary Graduates in Colorado',\n",
    "       'Post-Secondary Financial Aid Demographics in Colorado'],\n",
    "      'cdor',[\n",
    "      'revenue_marijuana',\n",
    "      ['Marijuana Sales by County in Colorado',\n",
    "       'State Retail Marijuana Sales Tax Revenue by County in Colorado',\n",
    "       'Marijuana Tax and Fee Revenue in Colorado',\n",
    "       'Marijuana Sales Revenue in Colorado'],\n",
    "      'retail_reports',\n",
    "      ['Retail Sales Tax Return History in Colorado',\n",
    "       'Retail Reports by City in Colorado',\n",
    "       'Retail Reports by County in Colorado',\n",
    "       'Retail Reports by Industry and City in Colorado',\n",
    "       'Retail Reports by Industry and County in Colorado',\n",
    "       'Retail Reports by Industry in Colorado'],\n",
    "      'regulations_liquor',\n",
    "      ['Liquor Permits for Special Events in Colorado',\n",
    "       'Liquor Compliance Check Statistics in Colorado',\n",
    "       'Liquor Licenses in Colorado',\n",
    "       'Recently Approved Liquor Licenses in Colorado',\n",
    "       'Recently Expired and Surrendered Liquor Licenses in Colorado',\n",
    "       'Sales Rooms in Colorado',\n",
    "       'Manufacturer Temporary Sales Room Permits in Colorado']\n",
    "      ],\n",
    "      'cdos',[\n",
    "      'health',\n",
    "      ['Durable Medical Equipment Suppliers in Colorado'],\n",
    "      'government',\n",
    "      ['Current Notaries in Colorado'],\n",
    "      'business',[\n",
    "      'ucc',\n",
    "      ['Uniform Commercial Code (UCC) Collateral Information in Colorado',\n",
    "       'Uniform Commercial Code (UCC) Debtor Information in Colorado',\n",
    "       'Uniform Commercial Code (UCC) Filing Information in Colorado',\n",
    "       'Secured Party Information in Colorado'],\n",
    "      'business',\n",
    "      ['Business Entities in Colorado',\n",
    "       'Business Entity Transaction History',\n",
    "       'Trademarks for Businesses in Colorado',\n",
    "       'Trade Names for Businesses in Colorado',\n",
    "       'Master List in Colorado'],\n",
    "      'nonprofit',\n",
    "      ['Federal Tax-Exempt Subsection Codes in Colorado',\n",
    "       'Registration for Charities, Paid Solicitors, Professional Fundraising Consultants, and for-profit Public Benefit Corporations in Colorado',\n",
    "       'Charitable Organizations Offices in Colorado',\n",
    "       'Other State Solicitation of Charities Registrants in Colorado',\n",
    "       'Charitable Purpose of the Charity in Colorado',\n",
    "       'Paid Solicitor Solicitation Notices in Colorado',\n",
    "       'Campaign Reports for Solicitation Notices to Charities in Colorado',\n",
    "       'Solicitation Campaign Supervisors Listed on Solicitation Notices in Colorado',\n",
    "       'Charity Extension Requests',\n",
    "       'Persons Associated with Charitable Organizations, Paid Solicitors, and Professional Fundraising Consultants in Colorado',\n",
    "       'Other Names a Registered Entity Uses to Solicit Contributions',\n",
    "       'Paid Solicitors Disclosed on Charity Registration Forms in Colorado',\n",
    "       'Charitable Solicitation Call Center Locations in Colorado',\n",
    "       'Charities Solicitation Type by Solicitation in Colorado',\n",
    "       'Communication Methods Used in Solicitation Campaigns in Colorado']\n",
    "      ],\n",
    "      'lobbyist',\n",
    "      ['Directory of Lobbyists in Colorado',\n",
    "       'Directory of Lobbyist Clients in Colorado',\n",
    "       'Expenses for Lobbyists in Colorado',\n",
    "       'Characterization of Lobbyist Clients in Colorado',\n",
    "       'Subcontractors for Lobbyists in Colorado',\n",
    "       'Bill Information and Position with Income of Lobbyist in Colorado']\n",
    "      ],\n",
    "      'cdot',[\n",
    "      'transportation_road_attributes',\n",
    "      ['Highway Milepoints in Colorado',\n",
    "       'Highway Mileposts in Colorado',\n",
    "       'Highway Routes in Colorado',\n",
    "       'Highway Routes in Colorado',\n",
    "       'Local Roads in Colorado',\n",
    "       'Major Roads in Colorado',\n",
    "       'Scenic Byways in Colorado'],\n",
    "      'tops',\n",
    "      ['CDOT Expenses', 'CDOT Revenues', 'CDOT Payroll'],\n",
    "      'natural_resources',\n",
    "      ['Lakes in Colorado', 'Streams in Colorado'],\n",
    "      'transportation_infrastructure',\n",
    "      ['Airports in Colorado',\n",
    "       'Cities in Colorado',\n",
    "       'Counties in Colorado',\n",
    "       'Railroads in Colorado']\n",
    "      ],\n",
    "      'ceo',[\n",
    "      'useia',\n",
    "      ['Gasoline Prices in Colorado', 'Natural Gas Prices in Colorado']\n",
    "      ],\n",
    "      'denver',\n",
    "      ['Temporary Outdoor Expansions for Restaurants in Denver, Colorado'],\n",
    "      'dola',[\n",
    "      'special_districts',\n",
    "      ['Metro Districts in Colorado',\n",
    "       'Parks and Rec Districts in Colorado',\n",
    "       'Fire Districts in Colorado',\n",
    "       'Hospital Districts in Colorado',\n",
    "       'Water and Sanitation Districts in Colorado',\n",
    "       'Library Districts in Colorado',\n",
    "       'School Districts in Colorado',\n",
    "       'Soil Districts in Colorado',\n",
    "       'Cemetery Districts in Colorado',\n",
    "       'All Special Districts in Colorado'],\n",
    "      'boundaries',\n",
    "      ['Municipal Annexations in Colorado', 'Municipal Boundaries in Colorado'],\n",
    "      'demographics',\n",
    "      ['Population Projections in Colorado',\n",
    "       'Race Estimates in Colorado',\n",
    "       'Race Forecast in Colorado']\n",
    "      ],\n",
    "      'dora',[\n",
    "      'regulations',\n",
    "      ['Licensed Real Estate Professionals in Colorado',\n",
    "       'Professional and Occupational Licenses in Colorado']\n",
    "      ],\n",
    "      'dpa',\n",
    "      ['DPA Tops Data'],\n",
    "      'irs',\n",
    "      ['Purpose and Operational Size of Charities Operating in Colorado',\n",
    "       'Fundraising Revenue of Charities Operating in Colorado',\n",
    "       'Total Revenue of Charities Operating in Colorado',\n",
    "       'IRS Filing Information for Charities Operating in Colorado',\n",
    "       'Total Revenue and Types of Art for Charities Operating in Colorado',\n",
    "       'Conservation Easements for Charities Operating in Colorado',\n",
    "       'Activities of Charities Operating in Colorado',\n",
    "       'Expenses of Charities Operating in Colorado',\n",
    "       'Expenses of Charities Operating in Colorado'],\n",
    "      'tchd',\n",
    "      ['Restaurant Inspections in Tri-County Colorado']]]\n",
    "\n",
    "    return sorted(datasets)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def map4x4(row):\n",
    "    \n",
    "    if isinstance(row[\"Data Link\"],str) and  len(row[\"Data Link\"]) == 9 and re.findall( \"\\w{4}-\\w{4}\",row[\"Data Link\"]):\n",
    "  #      link = '<a href=\"https://data.colorado.gov/dataset/{}\">{}</a>'.format(row[\"Data Link\"],row[\"Data Link\"])\n",
    "        \n",
    "        link = 'https://data.colorado.gov/dataset/{}'.format(row[\"Data Link\"])\n",
    "    else:\n",
    "        link = \"\"\n",
    "        \n",
    "    return link\n",
    "    \n",
    "#####################################################################\n",
    "\n",
    "def runETL(k):\n",
    "    global extract,a,string,dataSetsEtl\n",
    "    text2 = \"\"\n",
    "  #  print(k)\n",
    "    directory = \"\"\n",
    "    group = \"\"\n",
    "    if \"info\" in dataSetsEtl[k]:\n",
    "        if \"directory\" in dataSetsEtl[k][\"info\"]:\n",
    "            directory = dataSetsEtl[k][\"info\"][\"directory\"]\n",
    "            group = dataSetsEtl[k][\"info\"][\"group\"]\n",
    "            \n",
    "## Build Summary Text string\n",
    "    for ky1 in dataSetsEtl[k].keys():\n",
    "        text2+=f\"{ky1}:\\n\" \n",
    "        for k2,v2 in dataSetsEtl[k][ky1].items(): \n",
    "            sl = 10 - len(k2)\n",
    "            s = \" \"*sl\n",
    "        \n",
    "            if isinstance(v2,list) == False:\n",
    "                text2+=f\"    {k2:10s}{s} :  {v2}\\n\"\n",
    "            else:\n",
    "                text2+=f\"    {k2:10s}{s} : {v2[0]}\\n\"\n",
    "                if len(v2) > 1:\n",
    "                    for val in v2[1:]:\n",
    "                        text2+= f\"                   {s} : {val}\\n\"\n",
    "        text2+=f\"\\n\\n\"\n",
    "## Build actions\n",
    "    extract = \"\"\n",
    "    if logging > 1:\n",
    "        print(dataSetsEtl[ds])\n",
    "    if 'extract' in dataSetsEtl[ds]:\n",
    "        print(\"Extract Level 1\",dataSetsEtl[ds]['extract']['language'])\n",
    "        if ('language' in dataSetsEtl[ds]['extract'] and dataSetsEtl[ds]['extract']['language'] == 'node'):\n",
    "            pgm =  dataSetsEtl[ds]['extract']['file']\n",
    "            options=\"\"\n",
    "            print(\"Extract Level 2\")\n",
    "            \n",
    "            if 'options' in dataSetsEtl[ds]['extract']:\n",
    "\n",
    "                for opts in dataSetsEtl[ds]['extract']['options']:\n",
    "                    options+= f\" {opts}\" \n",
    "            extract = f\"node {bicHome}{pgm} {options}\"\n",
    "            if logging > 0:\n",
    "                print(\"Extract \",extract)\n",
    "    transform=\"\"\n",
    "    filetr = \"\"\n",
    "    \n",
    "    if 'transform' in dataSetsEtl[ds]:\n",
    "        \n",
    "        if dataSetsEtl[ds][\"transform\"][\"language\"] == \"node\":\n",
    "             ff=dataSetsEtl[ds][\"transform\"][\"file\"]\n",
    "             transform=f\"node {bicHome}{directory}/{ff}\"\n",
    "        elif dataSetsEtl[ds][\"transform\"][\"language\"] == \"py\":\n",
    "             ff=dataSetsEtl[ds][\"transform\"][\"file\"]\n",
    "             transform=f\"PIPENV_PIPFILE=/home/joe/bic_etl/cdor/regulations_liquor/scripts/Pipfile pipenv run python {bicHome}{directory}/{ff}\"\n",
    "            \n",
    "        options=\"\"\n",
    " #       filetr = \"\"\n",
    "        if 'options' in dataSetsEtl[ds]['transform']:\n",
    "            for opts in dataSetsEtl[ds]['transform']['options']:\n",
    "                options+= f\" {opts}\" \n",
    "                if isinstance(opts,str):\n",
    "                    spl = opts.split(\" \")\n",
    "                    if \"-i\" in spl[0]:\n",
    "                        filetr=spl[1]\n",
    "                        if filetr[-4:] == \"xlsx\":\n",
    "                            filetr=filetr.replace(\".xlsx\",\".csv\")\n",
    "                        elif filetr[-3:] == \"xls\":\n",
    "                            filetr=filetr.replace(\".xls\",\".csv\")\n",
    "                        print(\"FFOUT \",filetr)\n",
    "             #   print(\"TR OPTIONS\",opts)\n",
    "            transform+= f\" {options}\"\n",
    "            \n",
    "        else:\n",
    "            filetr=\"\"  \n",
    "        if logging > 0:\n",
    "            print(\"Transform: \",transform)\n",
    "        \n",
    "    text2+=f\"\\n\\nActions Strings\\nExtract\\n{extract}\\n\\nTransform\\n{transform}\"\n",
    "        \n",
    "    return text2,extract,transform,filetr\n",
    "    \n",
    "#############################################\n",
    "\n",
    "def setRowColors(lst,col1,col2,colsp,header):\n",
    "    count=0\n",
    "    colors = {}\n",
    "    # print(\"SRC head \",header)\n",
    "    # print(\"SRC list\",lst)\n",
    "\n",
    "    nrec = header.index(\"% Missing\")\n",
    "    for vals in lst:\n",
    "        key = vals[0]\n",
    "        if count%2 == 0:\n",
    "            colors[key] = col1\n",
    "        else:\n",
    "            colors[key] = col2\n",
    "        if vals[nrec] > 99.0:\n",
    "           \n",
    "            colors[key] = colsp\n",
    "        count+=1\n",
    "    colTab = []\n",
    "    for key,colr in colors.items():\n",
    "        colTab.append(colr)\n",
    "    rowNums = [num for num in range(0,len(colTab)+1)]\n",
    "    colText = [\"black\"]*len(colTab)\n",
    "    colrw = list(zip(rowNums,colTab))\n",
    "  \n",
    "    return colrw\n",
    "\n",
    "##########################################################\n",
    "\n",
    "def setRowColorsGeneric(lst,col1,col2):\n",
    "    count=0\n",
    "    rowNums=[]\n",
    "    colTab=[]\n",
    "\n",
    "    for vals in lst:       \n",
    "        if count%2 == 0:\n",
    "            colTab.append(col1)\n",
    "        else:\n",
    "            colTab.append(col2)\n",
    "        rowNums.append(count)\n",
    "        count+=1\n",
    "    colrw = list(zip(rowNums,colTab))\n",
    "  \n",
    "    return colrw\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def showDfRecs(df1,colUn,val,windowP,title=\"DF Unique Record Values\"):\n",
    "    global color1,color2\n",
    "    header_list = list(df1.columns)\n",
    "  \n",
    "    col_widths = [8]*len(header_list)\n",
    " #   col_widths[0] = 25\n",
    "    columnSortStateTable1 = {}\n",
    "  \n",
    "\n",
    "    ## set up sort state for the column in both tables\n",
    "    for col in header_list:\n",
    "        columnSortStateTable1[col] = -1\n",
    "       \n",
    "            \n",
    "#    valsFile1 = getValues(stats1,header_list)\n",
    "    values = df1.values.tolist()\n",
    "    \n",
    "    rowFile1Colors = setRowColorsGeneric(values,color1,color2)\n",
    "    \n",
    "    layout = [[sg.Text(f\"Column: \",font=\"CENTAUR 10\"),\n",
    "               sg.Text(f\"{colUn}\",font=\"CENTAUR 15\")],\n",
    "              [sg.Text(f\"Unique Value: \",font=\"CENTAUR 10\"),\n",
    "               sg.Text(f\"{val}\",font=\"CENTAUR 15\")],\n",
    "               [sg.Button(\"Quit\")],\n",
    "               [sg.Table(values=values,text_color=\"black\", auto_size_columns=False,enable_events=True,num_rows=20,col_widths=col_widths,font=\"CENTAUR 10\",\n",
    "                   justification='center',pad=(5,5),vertical_scroll_only=False,\n",
    "                   key='-TABLEFILE-',row_colors=rowFile1Colors,headings = header_list,metadata=columnSortStateTable1)]\n",
    "               ]\n",
    "\n",
    "    colorTheme = windowP.metadata[0]          \n",
    "#    sg.theme(colorTheme)     \n",
    "    window2 = sg.Window(title,layout,finalize=True,resizable=True,metadata=[colorTheme,df1])\n",
    "    a = window2.CurrentLocation()\n",
    "    screen_width, screen_height = window2.get_screen_dimensions()\n",
    "    win_width, win_height = window2.size\n",
    "    x, y = (screen_width - win_width)//2, (screen_height - win_height)//2\n",
    "    x=200\n",
    "    y=200\n",
    "    window2.move(x, y)\n",
    "    tableFile1 = window2['-TABLEFILE-']\n",
    "    tableFile1.bind('<Button-1>', \"Click\")\n",
    "    headerStore[tableFile1] = header_list\n",
    "    return window2,tableFile1,values,rowFile1Colors\n",
    "\n",
    "###################################################\n",
    "\n",
    "def splitL(data):\n",
    "    if data:\n",
    "        head, *tail = data  # This is a nicer way of doing head, tail = data[0], data[1:]\n",
    "        return {head: splitL(tail)}\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "#############################################\n",
    "\n",
    "def stringArgs(string):\n",
    "#  This is set to decode the extract string to get the location and name of the \n",
    "#  extracted data file\n",
    " #   print(\"stringArg\",string)\n",
    "    h =split(string)\n",
    " #   print(\"HHH \",h)\n",
    "    inf = h.index(\"-f\")\n",
    "    inf = h[inf+1]\n",
    "    if \"-a\" in h:\n",
    "        ot = h.index(\"-a\")\n",
    "        inf=inf.replace(inf[-4:],h[ot+1])\n",
    "        of = inf.split(\"/\")  \n",
    "        of=of[-1]\n",
    "    elif \"-n\" in h:\n",
    "        of = h.index(\"-n\")\n",
    "        of=h[of+1]\n",
    "        \n",
    "\n",
    "        \n",
    "#    ot = h[ot+1]\n",
    "    od = h.index(\"-o\")\n",
    "    od = h[od+1]\n",
    "\n",
    "    finalFile = f\"{bicHome}{od}{of}\"\n",
    "    print(\"finalFile \",finalFile)\n",
    "    return finalFile\n",
    "\n",
    "######################################################    \n",
    "\n",
    "def sortTable(row,stats,table,event,header):\n",
    "    \n",
    "        e = table.user_bind_event \n",
    "        region = table.Widget.identify('region', e.x, e.y)\n",
    "        if region == 'heading':\n",
    "            row = 0\n",
    "        elif region == 'cell':\n",
    "            row = int(table.Widget.identify_row(e.y))\n",
    "   \n",
    "        if row == 0:\n",
    "            colSortState = table.metadata\n",
    "            colClicked = int(table.Widget.identify_column(e.x)[1:])\n",
    "            header = headerStore[table]\n",
    "            \n",
    "            statClicked = header[colClicked-1].strip()\n",
    "           \n",
    "            colSortState[statClicked]*=-1\n",
    "            if colSortState[statClicked] == -1:\n",
    "                sortAsc=False\n",
    "            else:\n",
    "                sortAsc=True\n",
    "            if colClicked > 1:  # user number sort\n",
    "                statsS = dict(sorted(stats.items(), key=lambda x: x[1][statClicked],reverse=sortAsc))\n",
    "            else:\n",
    "                statsS = dict(sorted(stats.items(), key=lambda x: x[0],reverse=sortAsc))\n",
    "\n",
    "\n",
    "            statsVals=[]\n",
    "            for col in statsS:\n",
    "                 vals=[]\n",
    "                 vals.append(col)\n",
    "                 for k in header[1:]:\n",
    "                    vals.append(statsS[col][k.strip()])\n",
    "                 statsVals.append(vals)\n",
    "           # slen= len(statsVals)\n",
    "#             colorsTable = setRowColors(statsVals,\"#b3f0ff\",\"#33d6ff\",\"pink\",header_list)\n",
    "\n",
    "#             window['-TABLE-'].update(values=statsVals,row_colors=colorsTable)\n",
    "        \n",
    "        return statsVals,colSortState\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def showDFUN(col,unq,windowParent,colr1,colr2,colorTheme,title=\"\"):\n",
    "    global dataStore\n",
    "    valuesUNQ = list(zip(unq.index.tolist(),unq.tolist()))\n",
    "    hUNQ = []\n",
    "    hUNQ.append(\"Values\")\n",
    "    hUNQ.append(\"Count\")\n",
    "\n",
    "    \n",
    "    sortState = {}\n",
    "    for val in hUNQ:\n",
    "        sortState[val]=-1\n",
    "    layout2 = [    \n",
    "                   [sg.Text(f\"Showing unique Values for Column\"),sg.Text(f\" {col}\",text_color=\"white\",font='Courier 15 bold '),sg.Text(f\" and Reg Ex\",text_color=\"white\",font='Courier 10 bold ')],\n",
    "                \n",
    "                   [sg.Button('Quit')],\n",
    "            \n",
    "                   [sg.Button('Write Unique'),\n",
    "                    sg.Table(values=valuesUNQ,\n",
    "                       background_color=colr1,vertical_scroll_only=False,col_widths=60,font='Courier 10 bold ' ,\n",
    "                       auto_size_columns=True,enable_events=True,def_col_width=25,text_color=\"black\",\n",
    "                       justification='right',alternating_row_color=colr2,\n",
    "                       key='-TABLE-', headings = hUNQ,metadata=sortState)]\n",
    "            ]\n",
    "    sg.theme(colorTheme)    \n",
    "    window2 = sg.Window(f\"Unique for {title}\", layout2,finalize=True,resizable=True, grab_anywhere=False,metadata=[windowParent,col])\n",
    "\n",
    "    table = window2['-TABLE-']\n",
    "    table.bind('<Button-1>', \"Click\")\n",
    "    dataStore[table]=valuesUNQ\n",
    "    \n",
    "    return window2,table,valuesUNQ,hUNQ\n",
    "\n",
    "############################################################\n",
    "\n",
    "def showColAnal(xrefsO2T,xrefsT2O,colr1,colr2):\n",
    "    global dataStore\n",
    "    values=[]\n",
    "    header = [\"Original Column\",\"Transformed Column\"]\n",
    "    for col in sorted(xrefsO2T.keys()):\n",
    "        tmp = [col,xrefsO2T[col]]\n",
    "        values.append(tmp)\n",
    "    \n",
    "    rowColors = setRowColorsGeneric(values,colr1,colr2)\n",
    "    \n",
    "    sortState = {}\n",
    "    for val in header:\n",
    "        sortState[val]=-1\n",
    "        \n",
    "    \n",
    "    a = [[sg.Text(\"Columns in Program NOT in Data\",font='Courier 10 bold ',justification=\"left\")],\n",
    "         [sg.Listbox(\"\",font='Courier 15 bold ',horizontal_scroll=True ,size=(20,5),key=\"-PNOTD-\")]] \n",
    "    \n",
    "    d = sg.Column(a)\n",
    "    \n",
    "    b = [[sg.Text(\"Columns in Data NOT in Program\",font='Courier 10 bold ',justification=\"left\")],\n",
    "         [sg.Listbox(\"\",font='Courier 15 bold ' ,enable_events=True,horizontal_scroll=True ,size=(20,5),key=\"-DNOTP-\")]] \n",
    "    e = sg.Column(b)\n",
    "    \n",
    "    c = [d,sg.VerticalSeparator(color=\"black\"),e]\n",
    "    layout2 = [    \n",
    "                   [sg.Text(f\"Original COlumns Transformed to Columns\")],\n",
    "                   [sg.Button(\"Compare 2 Data\"),sg.Button(\"Get SIJ Cols\"),sg.Button(\"Full Data Column Map\")],\n",
    "                   [sg.Button('Quit')],\n",
    "                   [c],\n",
    "                   [ sg.Table(values=values,\n",
    "                       vertical_scroll_only=False,col_widths=60,font='Courier 10 bold ' ,\n",
    "                       auto_size_columns=True,enable_events=True,def_col_width=25,text_color=\"black\",\n",
    "                       justification='right',row_colors=rowColors,\n",
    "                       key='-TABLECOLUMN-', headings = header,num_rows=20,\n",
    "                       metadata=[values,rowColors,sortState])]\n",
    "                ]\n",
    "#    sg.theme(colorTheme)    \n",
    "    window2 = sg.Window(f\"Orig vs Trans Column Analysis\", layout2,finalize=True,resizable=True, grab_anywhere=False,metadata=\"nothing\")\n",
    "\n",
    "    table = window2['-TABLECOLUMN-']\n",
    "    table.bind('<Button-1>', \"Click\")\n",
    "    dataStore[table]=values\n",
    "    headerStore[table]=header\n",
    "    windowsStore[\"columnAnalysis\"] = window2   \n",
    "    return window2,table,values,header\n",
    "\n",
    "##########################################################\n",
    "\n",
    "def showFullDataColMap(cols,vals):\n",
    "    \n",
    "    layout2 = [    \n",
    "                   [sg.Text(f\"Full Data Columns Map\")],\n",
    "                   [sg.Button('Quit')],\n",
    "                   [sg.Button('Show Missing Fields')],\n",
    "                   [ sg.Table(values=vals,\n",
    "                       vertical_scroll_only=False,col_widths=60,font='Courier 15 bold ' ,\n",
    "                       auto_size_columns=True,enable_events=True,def_col_width=25,text_color=\"black\",\n",
    "                       justification='right',row_colors=rowColors,\n",
    "                       key='-TABLEDCMAP-', headings = cols,num_rows=20,\n",
    "                       metadata=[vals])]\n",
    "                ]\n",
    "        \n",
    "    window2 = sg.Window(f\"Orig vs Trans Column Analysis\", layout2,finalize=True,resizable=True, grab_anywhere=False,metadata=\"nothing\")\n",
    "\n",
    "    table = window2['-TABLEDCMAP-']\n",
    "    table.bind('<Button-1>', \"Click\")\n",
    "    dataStore[table]=values\n",
    "    headerStore[table]=cols\n",
    "    windowsStore[\"fullDataColMap\"] = window2  \n",
    "\n",
    "###########################################################\n",
    "\n",
    "def showMissingColumns():\n",
    "    global missed\n",
    "\n",
    "    header_list = [\"Column\",\"% Missing\",\"Missing\",\"string\",\"integer\",\"float\",\"boolean\"]\n",
    "    stats1Ex = {}\n",
    "    \n",
    "    stats1Tr = {}\n",
    "    stats1Ci = {}\n",
    "    \n",
    "    for col in missed['Extract - Data']:\n",
    "     #   print(col,stats1Extract[col])\n",
    "        stats1Ex[col]= stats1Extract[col]\n",
    "\n",
    "    \n",
    "    for col in missed['Transform - Data']:\n",
    "      #  print(col,stats1Transform[col])\n",
    "        stats1Tr[col]= stats1Transform[col]\n",
    "\n",
    "    for col in missed['CIM - Data']:\n",
    "      #  print(col,stats1Cim[col])\n",
    "        stats1Ci[col]= stats1Cim[col]\n",
    "  \n",
    "    valsExtract = getValues(stats1Ex,header_list)\n",
    "    valsTransform = getValues(stats1Tr,header_list)\n",
    "    valsCim = getValues(stats1Ci,header_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    layout = [\n",
    "               [sg.Button(\"Quit\")],\n",
    "               [sg.Listbox(values=missed[\"BIC Inventory\"],size=(10,5),font=\"CENTAUR 10\"),\n",
    "                sg.Listbox(values=missed[\"Transform Program\"],size=(10,5),font=\"CENTAUR 10\"),\n",
    "                sg.Listbox(values=missed[\"Transform - SIJ\"],size=(10,5),font=\"CENTAUR 10\")],\n",
    "                [sg.Table(values=valsExtract,text_color=\"black\", auto_size_columns=False,enable_events=True,num_rows=10,font=\"CENTAUR 10\",\n",
    "                    justification='center',pad=(5,5),vertical_scroll_only=False,\n",
    "                    key='-TABLEMISSEXTR-',headings = header_list)],\n",
    "                [sg.Table(values=valsTransform,text_color=\"black\", auto_size_columns=False,enable_events=True,num_rows=10,font=\"CENTAUR 10\",\n",
    "                    justification='center',pad=(5,5),vertical_scroll_only=False,\n",
    "                    key='-TABLEMISSTRANS-',headings = header_list)],\n",
    "                [sg.Table(values=valsCim,text_color=\"black\", auto_size_columns=False,enable_events=True,num_rows=10,font=\"CENTAUR 10\",\n",
    "                    justification='center',pad=(5,5),vertical_scroll_only=False,\n",
    "                    key='-TABLEMISSCIM-',headings = header_list)]\n",
    "               ]\n",
    "\n",
    "#    colorTheme = windowP.metadata[0]  \n",
    "    theme = \"LightBrown 3\"\n",
    "    sg.theme(\"LightBrown 3\")     \n",
    "    window2 = sg.Window(\"Missing Fields\",layout,finalize=True,resizable=True,metadata=[theme])\n",
    "    a = window2.CurrentLocation()\n",
    "    screen_width, screen_height = window2.get_screen_dimensions()\n",
    "    win_width, win_height = window2.size\n",
    "    x, y = (screen_width - win_width)//2, (screen_height - win_height)//2\n",
    "    x=200\n",
    "    y=200\n",
    "    window2.move(x, y)\n",
    "    tableex = window2['-TABLEMISSEXTR-']\n",
    "    tableex.bind('<Button-1>', \"Click\")\n",
    "    tabletr = window2['-TABLEMISSTRANS-']\n",
    "    tabletr.bind('<Button-1>', \"Click\")\n",
    "    tableci = window2['-TABLEMISSCIM-']\n",
    "    tableci.bind('<Button-1>', \"Click\")\n",
    "    \n",
    "    dataStore[tableex] = valsExtract\n",
    "    dataStore[tabletr] = valsTransform\n",
    "    dataStore[tableci] = valsCim\n",
    "    \n",
    "    \n",
    "    return window2,tableex,tabletr,tableci\n",
    "    # headerStore[tableFile1] = header_list\n",
    "    \n",
    " \n",
    "###########################################################\n",
    "\n",
    "def sortUniqe(table,window,dataStore,headerStore):\n",
    "    global color1,color2\n",
    "    try:\n",
    "        e = table.user_bind_event\n",
    "        region = table.Widget.identify('region', e.x, e.y)\n",
    "        sortAsc = {}\n",
    "        sortAsc[1] =False\n",
    "        sortAsc[-1]=True\n",
    "   #     print(\"R \",region)\n",
    "        if region == 'heading':\n",
    "            values = dataStore[table]\n",
    "            \n",
    "           \n",
    "            header = headerStore[table]\n",
    "            # print(\"SU header \",header)\n",
    "            column = int(table.Widget.identify_column(e.x)[1:])\n",
    "            col=header[column-1]\n",
    "            \n",
    "            sortState = table.metadata\n",
    "           \n",
    "            sortState[col]*=-1\n",
    "            table.metadata = sortState\n",
    "          \n",
    "            values = sorted(values, key=lambda element: (element[column-1]),reverse=sortAsc[sortState[col]]) \n",
    "#            rowFile1Colors = setRowColors(values,color1,color2,\"pink\",header)\n",
    "            rowFile1Colors = setRowColorsGeneric(values,color1,color2)\n",
    "\n",
    "            window[\"-TABLE-\"].update(values=values)\n",
    "            dataStore[table] = values\n",
    "    except Exception as err:\n",
    "        exceptionLog(err,inspect.currentframe().f_code.co_name)\n",
    "                    \n",
    "#####################################################        \n",
    "\n",
    "def tranfColXref(file):\n",
    "    global transformProgram\n",
    " #   fin = open(\"/home/joe/bic_etl/cdos/business/nonprofit/scripts/reg_finan.js\",\"r\")\n",
    "    fin = open(file,\"r\")\n",
    "    \n",
    "    lines = fin.readlines() \n",
    "    transformProgram = lines\n",
    "    fout = open(\"output.txt\",\"w\")\n",
    "    org = []\n",
    "    xrefsO2T = {}\n",
    "    xrefsT2O = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if re.findall(\"row\",line.lower()) and re.findall(\"=\",line.lower()) and not re.findall(\"^//\",line.lstrip()):\n",
    "            st1=line.find(\"[\")\n",
    "            ed1=line.find(\"]\")\n",
    "            st2=line.rfind(\"[\")\n",
    "            ed2=line.rfind(\"]\")\n",
    "            org.append(line[st2+2:ed2-1])\n",
    "    #        print(line)\n",
    "    #        print(line[st1+1:ed1],line[st2+1:ed2])\n",
    "            fout.write(f\"{line[st1+1:ed1]}  {line[st2+1:ed2]}\\n\")\n",
    "            og = line[st2+1:ed2].replace(\"'\",\"\")\n",
    "            og = og.replace('\"','')\n",
    "            \n",
    "            og = og.replace(\"].toLowerCase()\",\"\")\n",
    "\n",
    "\n",
    "            tr = line[st1+1:ed1].replace(\".toLowerCase()\",\"\")\n",
    "            tr = tr.replace('\"','')\n",
    "            tr = tr.replace(\"'\",\"\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            # tr = line[st1+1:ed1]\n",
    "            # og = line[st2+1:ed2]\n",
    "\n",
    "            xrefsT2O[tr]  = og\n",
    "            xrefsO2T[og]  = tr\n",
    "    return xrefsO2T,xrefsT2O\n",
    "                \n",
    "############################################################\n",
    "\n",
    "def wrap(string, lenght=60):\n",
    "    if isinstance(string,str):\n",
    "       return '\\n'.join(textwrap.wrap(string, lenght))\n",
    "    else:\n",
    "        return \"\"\n",
    "        \n",
    "############################################################    \n",
    "       \n",
    "def cronGUI():\n",
    "    global ds,text2,a,string,numWindows,color1,color2,file,transform,dataStore,statsStore\n",
    "    global windowsStore,xrefsBy4x4,groupMenu\n",
    "    global transformProgram,dnotpLines,rowToWrite\n",
    "    global extractColumns,transformColumns,cimColumns\n",
    "    global transformSijColumns,loadSijColumns,xrefsO2T,xrefsT2O\n",
    "    global dfExtract,dfTransform,dfCim,missed,fields\n",
    "    global stats1Extract,stats1Transform,stats1Cim\n",
    "    \n",
    "    dataStore = {}\n",
    "    statsStore = {}\n",
    "    windowsStore = {}\n",
    "    datasets = init()\n",
    "    cimColumns = []\n",
    "    transformColumns = []\n",
    "    extractColumns = []\n",
    "    transformSijColumns = []\n",
    "    loadSijColumns = []\n",
    "    \n",
    "    errorsByDate,errorsByName=getRecentErrorsNew()\n",
    "    \n",
    "##  get xrefs b/w 4x4 ids and datasert titles...yay   \n",
    "    xrefsBy4x4,xrefsByTitle,fields = getXrefs()\n",
    "\n",
    "    mf = pd.read_excel(\"BICDataInventoryandMetadata.xlsx\",skiprows=0,sheet_name=\"Inventory_Active\",engine=\"openpyxl\")\n",
    "    \n",
    "    mf[\"CIM Link\"] = mf.apply(map4x4,axis=1)\n",
    "\n",
    "    mf=mf.loc[~mf[\"Standardized Title for Dataset\"].isna()]\n",
    "    mfShort = mf[['Standardized Title for Dataset', 'Data Link','CIM Data Type','GoCodePublishYear','Standardized Short Description','CIM Link']]\n",
    "    mfShort[\"Standardized Title for Dataset\"] = mfShort[\"Standardized Title for Dataset\"].astype(str)   \n",
    "    \n",
    "    header_list = list(mfShort.columns)\n",
    "    # mfV = []\n",
    "    # a = [\"\",\"\",nan,nan,\"\"]\n",
    "    # mfV.append(a)\n",
    "    layout = [\n",
    "              [sg.Text(\"BIC Cron DataSets\")],\n",
    "              [sg.Combo(datasets,enable_events=True,key=\"-DATASET-\",font='Courier 10 bold ')],\n",
    "              [sg.ButtonMenu('Groups', menu_def=groupMenu, key='Group Menu')],\n",
    "              [sg.Text(\"4x4\",font='Courier 15 bold '),sg.Input(\"wwbh-7bpa\",size=[8,1],key=\"-4x4-\",font='Courier 10 bold '),\n",
    "               sg.Button(\"4x4\")],\n",
    "              [sg.Button('Close',font='Courier 15 bold '),sg.Button(\"Plot Crons\"),sg.Button('STDOUT',font='Courier 15 bold')],\n",
    "              [sg.Button('Recent Errors',font='Courier 15 bold '),sg.Button(\"Update Logs\",font='Courier 15 bold '),sg.Button(\"View Log Summary\",font='Courier 15 bold ')],\n",
    "              [sg.FilesBrowse(button_text=\"Local File\",initial_folder=\"/home/joe/bic_etl\",font=\"CENTAUR 15\",file_types=[(\"CSV Files\",\"*.csv\"),(\"TSV Files\",\"*.tsv\"),(\"Excel Files\",\"*.xlsx\")],enable_events=True,key='-FILE1-')],\n",
    "              [sg.Button(\"Column Analysis\",font='Courier 15 bold ')],\n",
    "   #          [sg.Button(\"GO\")],\n",
    "              [sg.Button(\"Extract\",font='Courier 10 bold ',key=\"-EXTRACT-\",visible=True),\n",
    "               sg.Button(\"Analyze-Extr\",font='Courier 10 bold ',key=\"-EXTRACTANAL-\",visible=False),\n",
    "               sg.Text(\"\",visible=False,font='Courier 15 bold ',key=\"-EXTRACTINFO-\")],\n",
    "              [sg.Button(\"Transform\",visible=True,font='Courier 10 bold ',key=\"-TRANSFORM-\"),\n",
    "               sg.Button(\"Ananlyze-Trans\",font='Courier 10 bold ',key=\"-TRANSFORMANAL-\",visible=False),\n",
    "               sg.Text(\"\",visible=False,font='Courier 15 bold ',key=\"-TRANSFORMINFO-\")\n",
    "              ],\n",
    "              [sg.Button(\"CIM API\",font='Courier 15 bold ',key=\"-CIMAPI-\",visible=False),sg.Text(\"\",visible=False,font='Courier 15 bold ',key=\"-CIMINFO-\")],\n",
    "              [sg.Multiline(default_text=\"Dataset Summary\",enable_events=True,font='Courier 15 bold ',background_color=\"blue\",key=\"-OUTPUT-\",size=[70,10]),\n",
    "               sg.Multiline(default_text=\"ETL Summary\",key=\"-ETL-\",size=[70,20],font='Courier 15 bold ')]\n",
    "            ]\n",
    "\n",
    "       \n",
    "    # Create the Window\n",
    "    sg.theme('Dark Green 5')\n",
    "    window2 = sg.Window('ETL', layout,finalize=True,resizable=True)\n",
    "    #window = sg.Window('Window Title', layout, web_port=2222, web_start_browser=False)\n",
    "    #table = window2['-TABLE2-']\n",
    "\n",
    "    #window2.move(window.current_location()[0]+600, window.current_location()[1])\n",
    "    try: \n",
    "        while True:\n",
    "\n",
    "         #   event, values = window2.read()\n",
    "            wid, event, values = sg.read_all_windows()\n",
    "            print('event: ',event)\n",
    " \n",
    "            if event == sg.WIN_CLOSED or event == 'Close':\n",
    "                window2.close()\n",
    "                break\n",
    "            elif event ==  event == 'Quit':\n",
    "                wid.close()\n",
    "### DATASET                \n",
    "            elif event == \"-DATASET-\" or event == \"4x4\" or event == \"Group Menu\":\n",
    "      #      elif event == \"GO\":#\n",
    "                if event == \"-DATASET-\":\n",
    "                    (ds)  = list(values.items())\n",
    "                    ds=ds[0][1]\n",
    "                elif event == \"4x4\":\n",
    "                    ds = xrefsBy4x4[values[\"-4x4-\"]]\n",
    "                  #  print(\"XREF DS \",values[\"-4x4-\"],ds)\n",
    "                elif event == \"Group Menu\":\n",
    "                    ds = values[\"Group Menu\"]\n",
    "                    \n",
    "  #              ds = \"Paid Solicitors Disclosed on Charity Registration Forms in Colorado\"\n",
    "                print(\"Daetaset \",ds)\n",
    "                datasetTitle = ds\n",
    "                mmf = mfShort.loc[mfShort[\"Standardized Title for Dataset\"].str.strip() == ds.strip()]\n",
    "                cim4x4 = mmf['Data Link'].values.tolist()[0]\n",
    "                cimApi = f\"https://data.colorado.gov/api/views/{cim4x4}/rows.csv?accessType=DOWNLOAD\"\n",
    "                if mmf.shape[0] > 0:\n",
    "                #            mmf[\"Standardized Short Description\"] = mmf[\"Standardized Short Description\"].map(wrap)          \n",
    "                    text = f\"Title             : {mmf['Standardized Title for Dataset'].values.tolist()[0]}\\nSocrata        : {mmf['Data Link'].values.tolist()[0]}\\nData Type    : {mmf['CIM Data Type'].values.tolist()[0]}\\nPublish Year: {mmf['GoCodePublishYear'].values.tolist()[0]}\\nCIM Link : {mmf['CIM Link'].values.tolist()[0]}\\nDescription  : {mmf['Standardized Short Description'].values.tolist()[0]}\"            \n",
    "                   # display(mmf)\n",
    "                else:\n",
    "                    text = \"None\"\n",
    "                if ds in dataSetsEtl:\n",
    "                      text2,extract,transform,trfile = runETL(ds)\n",
    "                else:\n",
    "                      text2=\"\"\n",
    "                      extract = \"\"\n",
    "                if len(extract) > 10:\n",
    "                    window2[\"-EXTRACT-\"].update(visible=True)\n",
    "                if len(transform) > 10:\n",
    "                    window2[\"-TRANSFORM-\"].update(visible=True)\n",
    "                 #   window2[\"-TRANSFORM-\"].update(visible=True)\n",
    "          \n",
    "                if len(cimApi) > 10:                 \n",
    "                    text2+=  f\"\\n\\nCIM API\\n{cimApi}\"\n",
    "                    window2[\"-CIMAPI-\"].update(visible=True)\n",
    "                    \n",
    "                   \n",
    "                window2[\"-OUTPUT-\"].update(text)\n",
    "                window2[\"-ETL-\"].update(text2)\n",
    "                link =  mmf['CIM Link'].values.tolist()[0]\n",
    "                string = \"\\n\\nCLI Update\\n\"\n",
    "                string+= f\"node /home/joe/bic_etl/general/scripts/bic_etl.js\"\n",
    "                string+= f\"\\n-t '{ds}'\\n\"\n",
    "                string+= \"-p ?\"\n",
    "                window2[\"-ETL-\"].update(string,append=True)\n",
    "                \n",
    "           #     print(\"Link \",mmf['CIM Link'],link.find(\"http\"))\n",
    "                \n",
    "                # if link.find(\"http\") > -1:\n",
    "                #      window2[\"-LINK-\"].update (visible=True)\n",
    "                # else:\n",
    "                #      window2[\"-LINK-\"].update (visible=False)\n",
    "### LINK                \n",
    "               \n",
    "            elif event == \"-LINK-\":\n",
    "                  #  print(\"Going To: \",link) \n",
    "                    if len(link) > 10:\n",
    "                        webbrowser.open(link)\n",
    "### Plot Crons                \n",
    "            elif event == \"Plot Crons\":    \n",
    "                plotCrons(crons_all)\n",
    "            \n",
    "### Output Columns\n",
    "            elif event == \"Output Columns\":\n",
    "                 df = wid.metadata[1]\n",
    "                 ctitle = wid.metadata[2]\n",
    "                 columns = sorted(list(df.columns))\n",
    "                 fout = open(f\"/tmp/columns.txt\",\"a+\")\n",
    "                 fout.write(f\"\\n{datetime.today()} -> {cim4x4} -> {ctitle} - {datasetTitle}\\n\")\n",
    "                 for col in columns:\n",
    "                     fout.write(col+\"\\n\")\n",
    "                 fout.close()\n",
    "                 print(f\"{len(columns)} columns written to /tmp/columns.txt\")\n",
    "### STDOUT\n",
    "            elif event == \"STDOUT\":\n",
    "                layouto = [[sg.Button(\"Quit\")],\n",
    "                          [sg.Multiline(s=(90,30),font='Courier 15 bold ',background_color=\"#F7DC6F\",text_color=\"black\",key=\"-STDOUT-\")]\n",
    "                          ] \n",
    "                wind = sg.Window('STDOUT STDERR', layouto,finalize=True,resizable=True)\n",
    "### View Log Summary\n",
    "            elif event == 'View Log Summary':\n",
    "                vals,cols = getLogSummary()\n",
    "                showLogSummary(vals,cols)\n",
    "        \n",
    "### Local File\n",
    "            elif event == '-FILE1-':\n",
    "                    file=values[\"-FILE1-\"]\n",
    "                    dfLocal,stats1Local = getFile(\"Local\",file,window2)\n",
    "                    LocalColumns = list(dfLocal.columns)\n",
    "                    color1 = colorPairs[numWindows][0]\n",
    "                    color2 = colorPairs[numWindows][1]\n",
    "                  \n",
    "                    numWindows+=1\n",
    "                    if numWindows > len(colorPairs):\n",
    "                        numWindows=0\n",
    "\n",
    "                    WindowC,tabl1,valsFile1,rowFile1Colors = compareWindow(stats1Local,dfLocal,file,\"Local File Analysis\",\"DarkPurple\")\n",
    "                    dataStore[tabl1]=valsFile1\n",
    "                    windowsOpen[\"main\"].append(WindowC)    \n",
    "        \n",
    "### Recent Errors\n",
    "            elif event == 'Recent Errors':\n",
    "                w = showRecentLogs(errorsByDate)\n",
    "            \n",
    "### Include Completed Errors\n",
    "            elif event == \"Include Completed\":\n",
    "                errorStatus = getErrorStatus()\n",
    "                values,rowColrs = orderErrors(errorsByDate,errorStatus,1)\n",
    "                wid[\"-DAILY-\"].update(values=values,row_colors=rowColrs)\n",
    "        \n",
    "            \n",
    "### Update Logs\n",
    "            elif event == \"Update Logs\":\n",
    "                nfiles,files = updateLogs(1)\n",
    "                print(f\"Downloaded {nfiles} for THIS month\")\n",
    "                nfiles,files = updateLogs(2)\n",
    "                print(f\"Downloaded {nfiles} for Last month\")\n",
    "                errorsByDate,errorsByName=getRecentErrorsNew()\n",
    "                string=f\"{nfiles} Downloaded\\n\"\n",
    "            \n",
    "### Daily-click\n",
    "            elif event == '-DAILY-Click':\n",
    "                xx=wid.metadata\n",
    "\n",
    "                table=wid[\"-DAILY-\"]\n",
    "                e = table.user_bind_event\n",
    "                region = table.Widget.identify('region', e.x, e.y)\n",
    "                if region == 'heading':\n",
    "                    row = 0\n",
    "                elif region == 'cell':\n",
    "                    row = int(table.Widget.identify_row(e.y))\n",
    "                elif region == 'separator':\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "             #   print(row,xx[row-1])\n",
    "\n",
    "                string = f\"Date: {xx[row-1][0]}\\n\\nDataset: {xx[row-1][1]}\\n\\nMessage: {xx[row-1][3]}\\n\"\n",
    "                showError(xx[row-1])\n",
    "\n",
    "### Show Missing Columns\n",
    "            elif event == \"Show Missing Fields\":\n",
    "                  window2,tableex,tabletr,tableci = showMissingColumns()\n",
    "            \n",
    "###  Write Log\n",
    "            elif event == \"Write Log\":\n",
    "                row=wid.metadata\n",
    "            #    print(row)\n",
    "                rowToWrite = [row[0],\"Joe Comeaux\",row[1],row[2],values[\"-ERRORTITLE-\"],\n",
    "                              row[4],values[\"-ERRORNOTES-\"],values[\"-ERRORJIRA-\"],row[-1]]\n",
    "                count=0\n",
    "                for val in rowToWrite:\n",
    "                  #  print(count,val)\n",
    "                    count+=1\n",
    "    #            yy = [date,errorsByDate[date][\"name\"][nn],errorsByDate[date][\"4x4\"][nn],errorsByDate[date][\"title\"][nn],msg,errorsByDate[date][\"stitle\"][nn]]\n",
    "                ret = toGoogleSheet(rowToWrite)\n",
    "                sg.Popup(f\"Result of {event}\\n{ret}\",font='Courier 10 bold ')\n",
    "           \n",
    "### Mark Done\n",
    "            elif event == \"Mark Done\" or event == \"Mark Skip\":\n",
    "                row=wid.metadata\n",
    "        \n",
    "                status=\"Fixed\"\n",
    "                if event == \"Mark Skip\":\n",
    "                    status=\"Skipped\"\n",
    "                toMark = [row[-1],row[1],row[3],row[2],status]\n",
    "                ret = markDone(toMark)\n",
    "                sg.Popup(f\"Result of {event}\\n{ret}\",font='Courier 10 bold ')\n",
    "\n",
    "                ### Full Data Column Map\n",
    "            elif event == \"Full Data Column Map\":\n",
    "                cols,vals,missed = fullDataColumnMap(cim4x4)\n",
    "                showFullDataColMap(cols,vals)\n",
    "            \n",
    "### Get SIJ Cols\n",
    "            elif event == \"Get SIJ Cols\":\n",
    "                getSijColumns(\"\")\n",
    "              #  print(\"Tr SIJ \",transformSijColumns)\n",
    "              #  print(\"LD SIJ \",loadSijColumns)\n",
    "                 \n",
    "### DNOTP\n",
    "\n",
    "            elif event == \"-DNOTP-\":\n",
    "                col = values[\"-DNOTP-\"][0]\n",
    "                if col in dnotpLines:\n",
    "                    string=\"\"\n",
    "                    string = \"\\n\".join(dnotpLines[col])\n",
    "                else:\n",
    "                    string=\"NOTHING FOUND\"\n",
    "                sg.Popup(string)\n",
    "                    \n",
    "             #   print(\"DNOTP STRai\",string)\n",
    "### Column Analysis                \n",
    "            elif event == \"Column Analysis\":\n",
    "           #     trfile = dataSetsEtl[ds][\"transform\"][\"file\"]\n",
    "                if transform[0:4] == \"node\":\n",
    "                    trfile = transform[5:]\n",
    "        \n",
    "                \n",
    "                xrefsO2T,xrefsT2O = tranfColXref(trfile)\n",
    "                color1 = colorPairs[numWindows][0]\n",
    "                color2 = colorPairs[numWindows][1]\n",
    "                \n",
    "                numWindows+=1\n",
    "                if numWindows > len(colorPairs):\n",
    "                    numWindows=0\n",
    "                showColAnal(xrefsO2T,xrefsT2O,color1,color2)\n",
    "             \n",
    "### Compare 2 Data                 \n",
    "            elif event == \"Compare 2 Data\":   \n",
    "              #  print(\"HERE We GO\")\n",
    "                ww = windowsStore[\"extract\"]\n",
    "                wdf = ww.metadata[1]\n",
    "                \n",
    "                compare2Data(wdf,xrefsO2T)\n",
    "                         \n",
    "### CIMAPI                \n",
    "            elif event == \"-CIMAPI-\":\n",
    "                  #  df1,stats1 = getFilesClicked(cimApi,window2)\n",
    "                    dfCim,stats1Cim = getFile(\"Fetch\",cimApi,window2)\n",
    "                    cimColumns = list(dfCim.columns)\n",
    "                    color1 = colorPairs[numWindows][0]\n",
    "                    color2 = colorPairs[numWindows][1]\n",
    "\n",
    "                    numWindows+=1\n",
    "                    if numWindows > len(colorPairs):\n",
    "                        numWindows=0\n",
    "\n",
    "                    WindowC,tabl1,valsFile1,rowFile1Colors = compareWindow(stats1Cim,dfCim,cimApi,\"CMI Analysis\",\"DarkRed\")\n",
    "                    dataStore[tabl1]=valsFile1\n",
    "                    \n",
    "                    windowsOpen[\"main\"].append(WindowC)  \n",
    "                    \n",
    "### TRANSFORM               \n",
    "            elif event == \"-TRANSFORM-\":\n",
    "                print(\"TRANSFORM \")\n",
    "                print(\"tr \",transform)\n",
    "                a=subprocess.run(transform,shell=True,capture_output=True,text=True)\n",
    "                string= \"\\n\\n-------------------------------------\\n\"\n",
    "                string+= \"TRANSFORM Output\\n\"\n",
    "                tfile=\"\"\n",
    "                if len(a.stderr) > 0:\n",
    "                    string+= f\"Error:\\n {a.stderr}\"\n",
    "                elif len(a.stdout) > 0 or len(trfile) > 0:\n",
    "                    if len(a.stdout) > 0:\n",
    "                        for msg in a.stdout.split(\"debug msg:\"):    \n",
    "                            if msg.find(\"final file is at:\") > -1:\n",
    "                                tmp = msg.split(\" \")\n",
    "                                tfile = tmp[-3]\n",
    "                            elif \"Output File\" in msg:\n",
    "                                il = msg.index(\"Output File\")\n",
    "                                tmp = msg[il+13:]\n",
    "                                tfile = tmp\n",
    "                            tfile=tfile.strip()\n",
    "                        #    print(f\"TTTFILE {tfile}:\")   \n",
    "                    elif len(trfile) > 0:\n",
    "                        tfile=trfile\n",
    "                    if os.path.isfile(tfile):\n",
    "\n",
    "                        window2[\"-TRANSFORMANAL-\"].update(visible=True)\n",
    "                        fileStats = os.stat(tfile)\n",
    "                    #    print(\"FSTATS \",fileStats)\n",
    "                        dt = datetime.fromtimestamp(fileStats.st_ctime)\n",
    "                        string = f\"{fileStats.st_size:,} bytes Created: {dt}  file: {tfile}\"\n",
    "                     #   print(\"STRING \",string)\n",
    "                        window2[\"-TRANSFORMINFO-\"].update(string,visible=True)\n",
    "                            \n",
    "                    string+=f\"STDOUT: {a.stdout}\"\n",
    "                    window2[\"-ETL-\"].update(string,append=True)\n",
    "                    \n",
    "### TRANSFORMANAL\n",
    "            elif event == \"-TRANSFORMANAL-\":\n",
    "                #    df1,stats1 = getFilesClicked(tfile,window2)\n",
    "                    dfTransform,stats1Transform = getFile(\"Local\",tfile,window2)\n",
    "                    transformColumns = list(dfTransform.columns)\n",
    "                    color1 = colorPairs[numWindows][0]\n",
    "                    color2 = colorPairs[numWindows][1]\n",
    "                  \n",
    "                    numWindows+=1\n",
    "                    if numWindows > len(colorPairs):\n",
    "                        numWindows=0\n",
    "\n",
    "                    WindowC,tabl1,valsFile1,rowFile1Colors = compareWindow(stats1Transform,dfTransform,tfile,\"Transform Analysis\",\"DarkPurple\")\n",
    "                    dataStore[tabl1]=valsFile1\n",
    "                    windowsOpen[\"main\"].append(WindowC)    \n",
    "                    \n",
    "### EXTRACT                  \n",
    "            elif event == \"-EXTRACT-\":\n",
    "                print(\"EXTACT: \",extract)\n",
    "                a=subprocess.run(extract,shell=True,capture_output=True,text=True)\n",
    "                print(\"RETURN \",a)\n",
    "                string= \"\\n\\n-------------------------------------\\n\"\n",
    "                string+= \"Extract Output\\n\"\n",
    "                if len(a.stderr) > 0:\n",
    "                    string+= \"fError:\\n {a.stderr}\"\n",
    "                string+=f\"STDOUT: {a.stdout}\"\n",
    "                window2[\"-ETL-\"].update(string,append=True)\n",
    "                sx = string.find(\"successfully download to\")\n",
    "                string[sx+25:]\n",
    "                sxo = string[sx+25:].find(\"!\")\n",
    "             \n",
    "                file=f\"/home/joe/bic_etl{string[sx+25:sx+25+sxo]}\"\n",
    "              \n",
    "                if os.path.isfile(file):\n",
    "                  \n",
    "                    window2[\"-EXTRACTANAL-\"].update(visible=True)\n",
    "                    fileStats = os.stat(file)\n",
    "                    dt = datetime.fromtimestamp(fileStats.st_ctime)\n",
    "                    string = f\"{fileStats.st_size:,} bytes Created: {dt}  file: {file}\"\n",
    "                    window2[\"-EXTRACTINFO-\"].update(string,visible=True)\n",
    "                    \n",
    "### EXTRACTANAL                      \n",
    "            elif event == \"-EXTRACTANAL-\":\n",
    "                file=stringArgs(extract)\n",
    "                \n",
    "             #   df1,stats1 = getFilesClicked(file,window2)\n",
    "                dfExtract,stats1Extract = getFile(\"Local\",file,window2)\n",
    "                extractColumns = list(dfExtract.columns)\n",
    "                color1 = colorPairs[numWindows][0]\n",
    "                color2 = colorPairs[numWindows][1]\n",
    "                \n",
    "                numWindows+=1\n",
    "                if numWindows > len(colorPairs):\n",
    "                    numWindows=0\n",
    "                    \n",
    "                WindowC,tabl1,valsFile1,rowFile1Colors = compareWindow(stats1Extract,dfExtract,file,\"Extract Analysis\",\"DarkBlue16\")\n",
    "                dataStore[tabl1]=valsFile1\n",
    "                windowsOpen[\"main\"].append(WindowC)\n",
    "                windowsStore[\"extract\"] = WindowC\n",
    "                \n",
    "### TABLEFILE-CLICK\n",
    "            elif event == \"-TABLEFILE-Click\":\n",
    "               \n",
    "                row,col=getRowClicked(tabl1,valsFile1)\n",
    "               \n",
    "                if row == 0:\n",
    "                   header = headerStore[tabl1]\n",
    "                   tabl1 = wid[\"-TABLEFILE-\"]\n",
    "                   stats1 = statsStore[tabl1]\n",
    "                   valsFile1,columnSortStateTable1 = sortTable(row,stats1,tabl1,event,header)          \n",
    "                   rowFile1Colors=setRowColors(valsFile1,color1,color2,\"pink\",header)\n",
    "                   wid[\"-TABLEFILE-\"].update(values=valsFile1,row_colors=rowFile1Colors)\n",
    "                   wid[\"-TABLEFILE-\"].metadata=columnSortStateTable1\n",
    "                else:\n",
    "                    df1 = wid.metadata[1]\n",
    "                    unq = df1[col].value_counts()\n",
    "                    colorTheme=wid.metadata[0]\n",
    "                    w,t,values,uHead = showDFUN(col,unq,wid,color1,color2,colorTheme)\n",
    "                  #dataStore[t] = values\n",
    "                    headerStore[t] = uHead\n",
    "                    windowsOpen[\"main\"].append(w)\n",
    "                    \n",
    "### TABLE-Click\n",
    "            elif event == \"-TABLE-Click\":  # This is the Unique Values tables\n",
    "                table = wid['-TABLE-']               \n",
    "               \n",
    "                \n",
    "                row,val =getRowClickedUN(table)\n",
    "                if row == 0:\n",
    "                    sortUniqe(table,wid,dataStore,headerStore)\n",
    "                else:\n",
    "                    widP = wid.metadata[0]\n",
    "                    col = wid.metadata[1]\n",
    "                    df = widP.metadata[1]\n",
    "                    tmp = df.loc[df[col] == val]\n",
    "                    showDfRecs(tmp,col,val,widP,title=\"DF Unique Record Values\")\n",
    "\n",
    "###  -TABLEMISSEXTR-Click                   \n",
    "            elif event in [\"-TABLEMISSEXTR-Click\",\"-TABLEMISSTRANS-Click\",\"-TABLEMISSCIM-Click\"] :\n",
    "                if event == \"-TABLEMISSEXTR-Click\":\n",
    "                   tabl1 = wid[\"-TABLEMISSEXTR-\"]\n",
    "                   title = \"EXTRACT\"\n",
    "                   df = dfExtract\n",
    "                elif event == \"-TABLEMISSTRANS-Click\":\n",
    "                   tabl1 = wid[\"-TABLEMISSTRANS-\"]\n",
    "                   df =  dfTransform\n",
    "                   title = \"TRANSFORM\"\n",
    "                elif event == \"-TABLEMISSCIM-Click\":\n",
    "                   tabl1 = wid[\"-TABLEMISSCIM-\"]\n",
    "                   df =  dfCim\n",
    "                   title=\"CIM\"\n",
    "                 \n",
    "                \n",
    "                vals = dataStore[tabl1]\n",
    "                row,col=getRowClicked(tabl1,vals)\n",
    "                unq = df[col].value_counts()\n",
    "                colorTheme=wid.metadata[0]\n",
    "                w,t,values,uHead = showDFUN(col,unq,wid,color1,color2,colorTheme,title)\n",
    "                  #dataStore[t] = values\n",
    "                headerStore[t] = uHead\n",
    "                windowsOpen[\"main\"].append(w)\n",
    "                   \n",
    "         \n",
    "    except  Exception as err: \n",
    "       exceptionLog(err,inspect.currentframe().f_code.co_name)\n",
    " \n",
    "    for wid in windowsOpen[\"main\"]:\n",
    "      #  print(\"window MA\",wid)\n",
    "        if wid:\n",
    "          #  print(\"cosing \",wid)\n",
    "            wid.close()\n",
    "            wid = None\n",
    "\n",
    "cronGUI()\n",
    "\n",
    "\n",
    "\n",
    "### Bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9b584-35ae-4a5e-b386-c094a32604d6",
   "metadata": {},
   "source": [
    "## Bottom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fileBrowser",
   "language": "python",
   "name": "filebrowser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
